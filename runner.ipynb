{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "# from transformers import BertTokenizer, TFBertModel\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "import torch.nn as nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('C:/Users/divya/OneDrive/Documents/UTAUSTIN/Semester 6/461p/ECE-461P-Term-Project/train_pp2.csv' ,delimiter=',')\n",
    "test=pd.read_csv('C:/Users/divya/OneDrive/Documents/UTAUSTIN/Semester 6/461p/ECE-461P-Term-Project/test_pp2.csv' ,delimiter=',')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_id</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>vec0</th>\n",
       "      <th>vec1</th>\n",
       "      <th>vec2</th>\n",
       "      <th>vec3</th>\n",
       "      <th>vec4</th>\n",
       "      <th>vec5</th>\n",
       "      <th>...</th>\n",
       "      <th>cat_Electronics</th>\n",
       "      <th>cat_Fashion</th>\n",
       "      <th>cat_Food and Beverage</th>\n",
       "      <th>cat_Furniture and Decor</th>\n",
       "      <th>cat_Health and Wellness</th>\n",
       "      <th>cat_Household Items</th>\n",
       "      <th>cat_Media</th>\n",
       "      <th>cat_Office Equipment</th>\n",
       "      <th>cat_Pet Care</th>\n",
       "      <th>cat_Toys and Hobbies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.007633</td>\n",
       "      <td>0.013877</td>\n",
       "      <td>0.001542</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>-0.002364</td>\n",
       "      <td>-0.014765</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013427</td>\n",
       "      <td>-0.005142</td>\n",
       "      <td>0.006128</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>-0.010722</td>\n",
       "      <td>0.013957</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001426</td>\n",
       "      <td>-0.006458</td>\n",
       "      <td>0.002439</td>\n",
       "      <td>0.008610</td>\n",
       "      <td>0.012070</td>\n",
       "      <td>-0.011089</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005893</td>\n",
       "      <td>-0.008465</td>\n",
       "      <td>0.012312</td>\n",
       "      <td>-0.003756</td>\n",
       "      <td>0.010533</td>\n",
       "      <td>-0.000997</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008761</td>\n",
       "      <td>0.012107</td>\n",
       "      <td>-0.013877</td>\n",
       "      <td>-0.008480</td>\n",
       "      <td>-0.004083</td>\n",
       "      <td>-0.001077</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_id  item_condition_id  price  shipping      vec0      vec1      vec2  \\\n",
       "0         0                  3   10.0         1 -0.007633  0.013877  0.001542   \n",
       "1         1                  3   52.0         0  0.013427 -0.005142  0.006128   \n",
       "2         2                  1   10.0         1  0.001426 -0.006458  0.002439   \n",
       "3         3                  1   35.0         1  0.005893 -0.008465  0.012312   \n",
       "4         4                  1   44.0         0  0.008761  0.012107 -0.013877   \n",
       "\n",
       "       vec3      vec4      vec5  ...  cat_Electronics  cat_Fashion  \\\n",
       "0  0.002168 -0.002364 -0.014765  ...                0            1   \n",
       "1  0.001119 -0.010722  0.013957  ...                1            0   \n",
       "2  0.008610  0.012070 -0.011089  ...                0            1   \n",
       "3 -0.003756  0.010533 -0.000997  ...                0            0   \n",
       "4 -0.008480 -0.004083 -0.001077  ...                0            1   \n",
       "\n",
       "   cat_Food and Beverage  cat_Furniture and Decor  cat_Health and Wellness  \\\n",
       "0                      0                        0                        0   \n",
       "1                      0                        0                        0   \n",
       "2                      0                        0                        0   \n",
       "3                      0                        1                        0   \n",
       "4                      0                        0                        0   \n",
       "\n",
       "   cat_Household Items  cat_Media  cat_Office Equipment  cat_Pet Care  \\\n",
       "0                    0          0                     0             0   \n",
       "1                    0          0                     0             0   \n",
       "2                    0          0                     0             0   \n",
       "3                    0          0                     0             0   \n",
       "4                    0          0                     0             0   \n",
       "\n",
       "   cat_Toys and Hobbies  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     0  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = train.drop(columns=['price'])\n",
    "y = (train['price']) # talk about this \n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1142652    18.0\n",
       "1142448    14.0\n",
       "220982     15.0\n",
       "479456     23.0\n",
       "1299063    36.0\n",
       "           ... \n",
       "259178     44.0\n",
       "1414414    26.0\n",
       "131932     51.0\n",
       "671155     10.0\n",
       "121958     10.0\n",
       "Name: price, Length: 1185328, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create data for pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train.astype(np.float32)).to(device)\n",
    "X_test_tensor = torch.tensor(X_test.astype(np.float32)).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values.astype(np.float32)).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.values.astype(np.float32)).to(device)\n",
    "\n",
    "# Reshape y tensors to have the correct shape (n_samples, 1)\n",
    "y_train_tensor = y_train_tensor.view(-1, 1)\n",
    "y_test_tensor = y_test_tensor.view(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(16 * num_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        return torch.exp(x)\n",
    "\n",
    "class RegularizedModel(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(RegularizedModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_features, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return  torch.exp(x)\n",
    "\n",
    "class ComplexModel(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(ComplexModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_features, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return  torch.exp(x)\n",
    "class EnhancedModel(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(EnhancedModel, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        ).to(device=device)\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4)\n",
    "        ).to(device=device)\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        ).to(device=device)\n",
    "        self.final_layer = nn.Linear(128, 1).to(device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x).to(device=device)\n",
    "        x = self.layer2(x).to(device=device)\n",
    "        x = self.layer3(x).to(device=device)\n",
    "        x = self.final_layer(x).to(device=device)\n",
    "        return  torch.exp(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def rmsle_loss(y_pred, y_true):\n",
    "    # Ensure the inputs are float tensors (required for MSELoss and logarithmic operations)\n",
    "    y_true = y_true.float()\n",
    "    y_pred = y_pred.float()\n",
    "\n",
    "    # Adjust predictions to ensure they are all positive (since log(0) is undefined)\n",
    "    y_pred = torch.clamp(y_pred, min=1e-6)  # Adding a small epsilon to avoid log(0)\n",
    "    \n",
    "    # Compute the RMSLE using the logarithmic differences\n",
    "    return torch.sqrt(torch.mean((torch.log1p(y_pred) - torch.log1p(y_true)) ** 2))\n",
    "\n",
    "def train_model(model, train_loader, test_loader, num_epochs=5):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbose=True, min_lr=1e-6)\n",
    "\n",
    "    # Initialize minimum loss to a large value\n",
    "    min_train_rmsle = float('inf')\n",
    "    min_test_rmsle = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = rmsle_loss(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_rmsle = total_train_loss / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                test_loss = rmsle_loss(outputs, targets)\n",
    "                total_test_loss += test_loss.item()\n",
    "\n",
    "        avg_test_rmsle = total_test_loss / len(test_loader)\n",
    "        \n",
    "        # Update the learning rate scheduler\n",
    "        scheduler.step(avg_test_rmsle)\n",
    "\n",
    "        if avg_train_rmsle < min_train_rmsle:\n",
    "            min_train_rmsle = avg_train_rmsle\n",
    "        if avg_test_rmsle < min_test_rmsle:\n",
    "            min_test_rmsle = avg_test_rmsle\n",
    "\n",
    "        print(f'Epoch {epoch+1}: Train RMSLE = {avg_train_rmsle}, Test RMSLE = {avg_test_rmsle}')\n",
    "    \n",
    "    print(f'Minimum Train RMSLE so far: {min_train_rmsle}')\n",
    "    print(f'Minimum Test RMSLE so far: {min_test_rmsle}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X_train.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train RMSLE = 5137485853381.73, Test RMSLE = 0.784662239039544\n",
      "Epoch 2: Train RMSLE = 0.7680806976261667, Test RMSLE = 0.747738756775161\n",
      "Epoch 3: Train RMSLE = 0.7413838792037748, Test RMSLE = 0.7395818479643916\n",
      "Epoch 4: Train RMSLE = 0.7289735549106163, Test RMSLE = 0.7160217967282565\n",
      "Epoch 5: Train RMSLE = 2.8293548017091776, Test RMSLE = 0.7076882655317854\n",
      "Minimum Train RMSLE so far: 0.7289735549106163\n",
      "Minimum Test RMSLE so far: 0.7076882655317854\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "model = CNNModel(num_features).to(device)\n",
    "train_model(model, train_loader, test_loader)\n",
    "# Minimum Train RMSLE so far: 0.7004365426701981\n",
    "# Minimum Test RMSLE so far: 0.698955543707679"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = RegularizedModel(num_features).to(device)\n",
    "train_model(model2, train_loader, test_loader)\n",
    "# doesnt work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train RMSLE = 0.7043774527888352, Test RMSLE = 0.6922612426103874\n",
      "Epoch 2: Train RMSLE = 0.6906035386291811, Test RMSLE = 0.6879250371631189\n",
      "Epoch 3: Train RMSLE = 0.6895467307428441, Test RMSLE = 0.6880920242218136\n",
      "Epoch 4: Train RMSLE = 0.6890383984353101, Test RMSLE = 0.688188678230845\n",
      "Epoch 5: Train RMSLE = 0.6887633485748627, Test RMSLE = 0.6880506542608821\n",
      "Minimum Train RMSLE so far: 0.6887633485748627\n",
      "Minimum Test RMSLE so far: 0.6879250371631189\n"
     ]
    }
   ],
   "source": [
    "model3 = ComplexModel(num_features).to(device)\n",
    "train_model(model3, train_loader, test_loader)\n",
    "# Minimum Train RMSLE so far: 0.688113078240873\n",
    "# Minimum Test RMSLE so far: 0.687606498730978\n",
    "\n",
    "# Minimum Train RMSLE so far: 0.688113078240873\n",
    "# Minimum Test RMSLE so far: 0.687606498730978"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train RMSLE = 0.7159452734736425, Test RMSLE = 0.688680603794546\n",
      "Epoch 2: Train RMSLE = 0.6935721546448523, Test RMSLE = 0.6879314284774094\n",
      "Epoch 3: Train RMSLE = 0.6920516048398906, Test RMSLE = 0.6879945102828905\n",
      "Epoch 4: Train RMSLE = 0.6916209922841067, Test RMSLE = 0.6877204380483819\n",
      "Epoch 5: Train RMSLE = 0.6913184327930799, Test RMSLE = 0.6879418392375488\n",
      "Epoch 6: Train RMSLE = 0.6912575840505898, Test RMSLE = 0.6878238535743376\n",
      "Epoch 7: Train RMSLE = 0.6909255641713963, Test RMSLE = 0.6880650057748388\n",
      "Epoch 8: Train RMSLE = 0.690915531024299, Test RMSLE = 0.6879392043673125\n",
      "Epoch 9: Train RMSLE = 0.6908022080224827, Test RMSLE = 0.6896664626370288\n",
      "Epoch 10: Train RMSLE = 0.6908659141915617, Test RMSLE = 0.6877503546054974\n",
      "Epoch 11: Train RMSLE = 0.6907172332162843, Test RMSLE = 0.6877906458330474\n",
      "Epoch 12: Train RMSLE = 0.6905958335614347, Test RMSLE = 0.6876177308601119\n",
      "Epoch 13: Train RMSLE = 0.6905114932352714, Test RMSLE = 0.6875397739825355\n",
      "Epoch 14: Train RMSLE = 0.6902928074124783, Test RMSLE = 0.6876251977737458\n",
      "Epoch 15: Train RMSLE = 0.6903841229013152, Test RMSLE = 0.6875104146986022\n",
      "Epoch 16: Train RMSLE = 0.6903662268054466, Test RMSLE = 0.6876675814769871\n",
      "Epoch 17: Train RMSLE = 0.6902263001179529, Test RMSLE = 0.6875130934067613\n",
      "Epoch 18: Train RMSLE = 0.6902337238762988, Test RMSLE = 0.6879742950670819\n",
      "Epoch 19: Train RMSLE = 0.690395049397203, Test RMSLE = 0.687569538509838\n",
      "Epoch 20: Train RMSLE = 0.6902502480451621, Test RMSLE = 0.6877003756475047\n",
      "Epoch 21: Train RMSLE = 0.6900543204365561, Test RMSLE = 0.6877552104662984\n",
      "Epoch 22: Train RMSLE = 0.6900201162817051, Test RMSLE = 0.6876143839003535\n",
      "Epoch 23: Train RMSLE = 0.690048798119801, Test RMSLE = 0.6875502098600285\n",
      "Epoch 24: Train RMSLE = 0.6899916964050735, Test RMSLE = 0.6873892872425167\n",
      "Epoch 25: Train RMSLE = 0.689996232800952, Test RMSLE = 0.6875069513997412\n",
      "Minimum Train RMSLE so far: 0.6899916964050735\n",
      "Minimum Test RMSLE so far: 0.6873892872425167\n"
     ]
    }
   ],
   "source": [
    "model4 = EnhancedModel(num_features).to(device)\n",
    "train_model(model4, train_loader, test_loader)\n",
    "# Minimum Train RMSLE so far: 0.69227513503413\n",
    "# Minimum Test RMSLE so far: 0.6879885306087536\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finished Base neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_features, num_features)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(num_features, num_features)\n",
    "        self.bn = nn.BatchNorm1d(num_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.bn(out)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return  out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.block1 = ResidualBlock(num_features)\n",
    "        self.block2 = ResidualBlock(num_features)\n",
    "        self.final_layer = nn.Linear(num_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.final_layer(x)\n",
    "        return  x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train RMSLE = 1.426390644467216, Test RMSLE = 1.0603972458087763\n",
      "Epoch 2: Train RMSLE = 0.7680296310273471, Test RMSLE = 0.6909068401020461\n",
      "Epoch 3: Train RMSLE = 0.6924842647304223, Test RMSLE = 0.6904345765095297\n",
      "Epoch 4: Train RMSLE = 0.6919504568422467, Test RMSLE = 0.690175749543315\n",
      "Epoch 5: Train RMSLE = 0.6916262712503611, Test RMSLE = 0.6896854765380183\n",
      "Epoch 6: Train RMSLE = 0.6913382063403216, Test RMSLE = 0.6895079688798323\n",
      "Epoch 7: Train RMSLE = 0.6912930727336482, Test RMSLE = 0.6895803656067866\n",
      "Epoch 8: Train RMSLE = 0.6911249879475488, Test RMSLE = 0.6890395522774025\n",
      "Epoch 9: Train RMSLE = 0.6908792133906331, Test RMSLE = 0.6891707153550443\n",
      "Epoch 10: Train RMSLE = 0.6908167007674757, Test RMSLE = 0.689398987861875\n",
      "Minimum Train RMSLE so far: 0.6908167007674757\n",
      "Minimum Test RMSLE so far: 0.6890395522774025\n"
     ]
    }
   ],
   "source": [
    "model5 = ResidualBlock(num_features).to(device)\n",
    "train_model(model5, train_loader, test_loader)\n",
    "#Epoch 2, RMSLE: 0.7510158108007159\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = ResNet(num_features).to(device)\n",
    "train_model(model6, train_loader, test_loader)\n",
    "#Epoch 2, RMSLE: 0.7510158108007159"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_features, 512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.fc4 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc4(x)\n",
    "        return torch.exp(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train RMSLE = 0.7789893851209541, Test RMSLE = 0.6917615989659522\n",
      "Epoch 2: Train RMSLE = 0.6964964387445288, Test RMSLE = 0.6951838828425406\n",
      "Epoch 3: Train RMSLE = 19.46874885803892, Test RMSLE = 0.6905204756127091\n",
      "Epoch 4: Train RMSLE = 0.6960191253654566, Test RMSLE = 0.6944242547099468\n",
      "Epoch 5: Train RMSLE = 0.6958181958599924, Test RMSLE = 0.6898327518606052\n",
      "Minimum Train RMSLE so far: 0.6958181958599924\n",
      "Minimum Test RMSLE so far: 0.6898327518606052\n"
     ]
    }
   ],
   "source": [
    "model7 = MLP(num_features).to(device)\n",
    "train_model(model7, train_loader, test_loader)\n",
    "\n",
    "# Minimum Train RMSLE so far: 0.6939673007973121\n",
    "# Minimum Test RMSLE so far: 0.6892061752535257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AdvancedMLP(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(AdvancedMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_features, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        \n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.dropout2 = nn.Dropout(0.05)\n",
    "        \n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.dropout3 = nn.Dropout(0.01)\n",
    "        \n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.final_layer = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.leaky_relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.leaky_relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        x = F.leaky_relu(self.bn4(self.fc4(x)))\n",
    "        x = self.final_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train RMSLE = 0.7023633642588892, Test RMSLE = 0.6903367664844371\n",
      "Epoch 2: Train RMSLE = 0.6899257162323872, Test RMSLE = 0.6880816763612422\n",
      "Epoch 3: Train RMSLE = 0.6892484756687741, Test RMSLE = 0.6876663033235307\n",
      "Epoch 4: Train RMSLE = 0.6888325244814253, Test RMSLE = 0.6893707580866039\n",
      "Epoch 5: Train RMSLE = 0.6885519486241887, Test RMSLE = 0.6881060414490857\n",
      "Epoch 6: Train RMSLE = 0.6883642813243617, Test RMSLE = 0.6883526197122822\n",
      "Epoch 7: Train RMSLE = 0.6875517218975171, Test RMSLE = 0.6877412989790305\n",
      "Epoch 8: Train RMSLE = 0.6873011699644409, Test RMSLE = 0.6875942824881012\n",
      "Epoch 9: Train RMSLE = 0.6870216123711875, Test RMSLE = 0.6877278189400673\n",
      "Epoch 10: Train RMSLE = 0.6868236210261776, Test RMSLE = 0.687866064763898\n",
      "Epoch 11: Train RMSLE = 0.6865404002864458, Test RMSLE = 0.688016016831848\n",
      "Epoch 12: Train RMSLE = 0.6858599585614292, Test RMSLE = 0.6880781474921235\n",
      "Epoch 13: Train RMSLE = 0.6855883740862582, Test RMSLE = 0.6882511685461391\n",
      "Epoch 14: Train RMSLE = 0.6853342313925074, Test RMSLE = 0.6884172019240821\n",
      "Epoch 15: Train RMSLE = 0.6846597515061481, Test RMSLE = 0.6886794951236986\n",
      "Epoch 16: Train RMSLE = 0.6845767071634445, Test RMSLE = 0.6891158833935667\n",
      "Epoch 17: Train RMSLE = 0.6845413493508944, Test RMSLE = 0.6888614541200808\n",
      "Epoch 18: Train RMSLE = 0.6840886051891935, Test RMSLE = 0.6890176540315859\n",
      "Epoch 19: Train RMSLE = 0.6838687935704373, Test RMSLE = 0.6892123494890127\n",
      "Epoch 20: Train RMSLE = 0.6838877420128437, Test RMSLE = 0.6892288710509333\n",
      "Epoch 21: Train RMSLE = 0.6837344641665146, Test RMSLE = 0.6892462102300121\n",
      "Epoch 22: Train RMSLE = 0.683713541961078, Test RMSLE = 0.6893183741564886\n",
      "Epoch 23: Train RMSLE = 0.6834786682765857, Test RMSLE = 0.6893401068581589\n",
      "Epoch 24: Train RMSLE = 0.6834767628958941, Test RMSLE = 0.6893035049148626\n",
      "Epoch 25: Train RMSLE = 0.683476001930175, Test RMSLE = 0.6894076453303446\n",
      "Epoch 26: Train RMSLE = 0.6834107924792404, Test RMSLE = 0.6894994644644038\n",
      "Epoch 27: Train RMSLE = 0.683464946702286, Test RMSLE = 0.689369396247248\n",
      "Epoch 28: Train RMSLE = 0.6835294968124394, Test RMSLE = 0.6894678463240315\n",
      "Epoch 29: Train RMSLE = 0.6833851979274573, Test RMSLE = 0.6896134300961451\n",
      "Epoch 30: Train RMSLE = 0.6833103905614409, Test RMSLE = 0.6894339634259611\n",
      "Epoch 31: Train RMSLE = 0.6833353842893293, Test RMSLE = 0.6895961284804771\n",
      "Epoch 32: Train RMSLE = 0.6832829858432108, Test RMSLE = 0.6894348821496377\n",
      "Epoch 33: Train RMSLE = 0.6833617386724089, Test RMSLE = 0.6895559852284403\n",
      "Epoch 34: Train RMSLE = 0.6832631998028407, Test RMSLE = 0.6895461610740216\n",
      "Epoch 35: Train RMSLE = 0.683321937237205, Test RMSLE = 0.6895297999529776\n",
      "Epoch 36: Train RMSLE = 0.6833118626362213, Test RMSLE = 0.689586034760159\n",
      "Epoch 37: Train RMSLE = 0.6832500279004448, Test RMSLE = 0.6894989685398588\n",
      "Epoch 38: Train RMSLE = 0.6832688571319525, Test RMSLE = 0.6896565225038898\n",
      "Epoch 39: Train RMSLE = 0.6832289892566056, Test RMSLE = 0.6895752776276599\n",
      "Epoch 40: Train RMSLE = 0.6833197538699247, Test RMSLE = 0.6896119295096351\n",
      "Epoch 41: Train RMSLE = 0.6832602716719678, Test RMSLE = 0.6895789157955571\n",
      "Epoch 42: Train RMSLE = 0.6832168440113096, Test RMSLE = 0.6896047569060012\n",
      "Epoch 43: Train RMSLE = 0.6833168627936319, Test RMSLE = 0.689563197274621\n",
      "Epoch 44: Train RMSLE = 0.68318839253498, Test RMSLE = 0.6894588683985449\n",
      "Epoch 45: Train RMSLE = 0.6831658782875826, Test RMSLE = 0.6895310558721381\n",
      "Epoch 46: Train RMSLE = 0.683219912734932, Test RMSLE = 0.6896201049604521\n",
      "Epoch 47: Train RMSLE = 0.6833148093104755, Test RMSLE = 0.6895344925799397\n",
      "Epoch 48: Train RMSLE = 0.6832008297999113, Test RMSLE = 0.689598710548283\n",
      "Epoch 49: Train RMSLE = 0.683239042866752, Test RMSLE = 0.6896385615818239\n",
      "Epoch 50: Train RMSLE = 0.6833617351082307, Test RMSLE = 0.6895241806803654\n",
      "Epoch 51: Train RMSLE = 0.683241302251597, Test RMSLE = 0.6895044589243448\n",
      "Epoch 52: Train RMSLE = 0.6832632590727906, Test RMSLE = 0.6895065858551503\n",
      "Epoch 53: Train RMSLE = 0.683333807158989, Test RMSLE = 0.6895908972815956\n",
      "Epoch 54: Train RMSLE = 0.6832834898807608, Test RMSLE = 0.6895115389547963\n",
      "Epoch 55: Train RMSLE = 0.6833358816587283, Test RMSLE = 0.6895618361109819\n",
      "Epoch 56: Train RMSLE = 0.6833032096322441, Test RMSLE = 0.6895209588118492\n",
      "Epoch 57: Train RMSLE = 0.683325932536126, Test RMSLE = 0.6895076959160434\n",
      "Epoch 58: Train RMSLE = 0.6833033038005671, Test RMSLE = 0.6896115738059501\n",
      "Epoch 59: Train RMSLE = 0.6831783217073587, Test RMSLE = 0.689452949127042\n",
      "Epoch 60: Train RMSLE = 0.6832438462477357, Test RMSLE = 0.6896719165199443\n",
      "Epoch 61: Train RMSLE = 0.6833016565597042, Test RMSLE = 0.6895145306227352\n",
      "Epoch 62: Train RMSLE = 0.6832771109721102, Test RMSLE = 0.689555758258432\n",
      "Epoch 63: Train RMSLE = 0.68320098342645, Test RMSLE = 0.6895725196418482\n",
      "Epoch 64: Train RMSLE = 0.6832980622316333, Test RMSLE = 0.6895750945341867\n",
      "Epoch 65: Train RMSLE = 0.6831835794124456, Test RMSLE = 0.6895034803193013\n",
      "Epoch 66: Train RMSLE = 0.683226659115985, Test RMSLE = 0.6896612759775282\n",
      "Epoch 67: Train RMSLE = 0.6831879745043481, Test RMSLE = 0.6895388790042888\n",
      "Epoch 68: Train RMSLE = 0.6832756281452581, Test RMSLE = 0.6894843701705179\n",
      "Epoch 69: Train RMSLE = 0.6833248120984984, Test RMSLE = 0.6895696572674657\n",
      "Epoch 70: Train RMSLE = 0.6831819985634526, Test RMSLE = 0.6895844612863454\n",
      "Epoch 71: Train RMSLE = 0.6831457212814328, Test RMSLE = 0.6895581958966285\n",
      "Epoch 72: Train RMSLE = 0.6832316842194246, Test RMSLE = 0.6895659786337938\n",
      "Epoch 73: Train RMSLE = 0.6832040237508938, Test RMSLE = 0.6895098904378212\n",
      "Epoch 74: Train RMSLE = 0.6831800724300721, Test RMSLE = 0.6896531073932796\n",
      "Epoch 75: Train RMSLE = 0.6832042803926412, Test RMSLE = 0.689595584007299\n",
      "Epoch 76: Train RMSLE = 0.6832707337015334, Test RMSLE = 0.6896721230189554\n",
      "Epoch 77: Train RMSLE = 0.6832905219946571, Test RMSLE = 0.6895237454158702\n",
      "Epoch 78: Train RMSLE = 0.6831541444659136, Test RMSLE = 0.6895683383393355\n",
      "Epoch 79: Train RMSLE = 0.6831765906930221, Test RMSLE = 0.6895968350226872\n",
      "Epoch 80: Train RMSLE = 0.6831904482982449, Test RMSLE = 0.6896290586502345\n",
      "Epoch 81: Train RMSLE = 0.6832119661961252, Test RMSLE = 0.6895385675696981\n",
      "Epoch 82: Train RMSLE = 0.683238405530547, Test RMSLE = 0.6896673335650136\n",
      "Epoch 83: Train RMSLE = 0.6832719107926983, Test RMSLE = 0.6895668474445331\n",
      "Epoch 84: Train RMSLE = 0.6832585432627798, Test RMSLE = 0.689661304698704\n",
      "Epoch 85: Train RMSLE = 0.6832203769566695, Test RMSLE = 0.6895439735539555\n",
      "Epoch 86: Train RMSLE = 0.6831679466356952, Test RMSLE = 0.6895675368171046\n",
      "Epoch 87: Train RMSLE = 0.6831670717691398, Test RMSLE = 0.6895367029907121\n",
      "Epoch 88: Train RMSLE = 0.6831670489149535, Test RMSLE = 0.689655574338273\n",
      "Epoch 89: Train RMSLE = 0.6832282259721593, Test RMSLE = 0.689560587843508\n",
      "Epoch 90: Train RMSLE = 0.683332672014098, Test RMSLE = 0.6896080372851447\n",
      "Epoch 91: Train RMSLE = 0.6832512796301672, Test RMSLE = 0.6896977642202347\n",
      "Epoch 92: Train RMSLE = 0.683288798741057, Test RMSLE = 0.6895592252576462\n",
      "Epoch 93: Train RMSLE = 0.6831168136160418, Test RMSLE = 0.6896418708365928\n",
      "Epoch 94: Train RMSLE = 0.6830796890781814, Test RMSLE = 0.6895531097644607\n",
      "Epoch 95: Train RMSLE = 0.6831635829455702, Test RMSLE = 0.6895773625288893\n",
      "Epoch 96: Train RMSLE = 0.6831460453529259, Test RMSLE = 0.6895698066008474\n",
      "Epoch 97: Train RMSLE = 0.6831939596107329, Test RMSLE = 0.6895931651540257\n",
      "Epoch 98: Train RMSLE = 0.68314648937229, Test RMSLE = 0.6895262246009882\n",
      "Epoch 99: Train RMSLE = 0.6831169384314698, Test RMSLE = 0.6895990193314904\n",
      "Epoch 100: Train RMSLE = 0.6830989266084251, Test RMSLE = 0.6896040909196615\n",
      "Epoch 101: Train RMSLE = 0.6831535362787325, Test RMSLE = 0.689580156861822\n",
      "Epoch 102: Train RMSLE = 0.6830595628768295, Test RMSLE = 0.6897277568051275\n",
      "Epoch 103: Train RMSLE = 0.6831514472085041, Test RMSLE = 0.689644728583925\n",
      "Epoch 104: Train RMSLE = 0.6831524205928062, Test RMSLE = 0.6896397387317282\n",
      "Epoch 105: Train RMSLE = 0.683260375633333, Test RMSLE = 0.6895797546945723\n",
      "Epoch 106: Train RMSLE = 0.683205594708689, Test RMSLE = 0.689536111075789\n",
      "Epoch 107: Train RMSLE = 0.6831113353245305, Test RMSLE = 0.6896447576075644\n",
      "Epoch 108: Train RMSLE = 0.6831784304928351, Test RMSLE = 0.6895272032317732\n",
      "Epoch 109: Train RMSLE = 0.6830457339974345, Test RMSLE = 0.6896330132144132\n",
      "Epoch 110: Train RMSLE = 0.6831622942771235, Test RMSLE = 0.6896257222509621\n",
      "Epoch 111: Train RMSLE = 0.6831263675750715, Test RMSLE = 0.6896270576987558\n",
      "Epoch 112: Train RMSLE = 0.683174571070363, Test RMSLE = 0.6897200636808621\n",
      "Epoch 113: Train RMSLE = 0.6831892800813205, Test RMSLE = 0.6895912410154427\n",
      "Epoch 114: Train RMSLE = 0.6831004559498138, Test RMSLE = 0.6895900740978194\n",
      "Epoch 115: Train RMSLE = 0.6831855049488463, Test RMSLE = 0.6896079127151696\n",
      "Epoch 116: Train RMSLE = 0.6830905669065436, Test RMSLE = 0.6895832790203008\n",
      "Epoch 117: Train RMSLE = 0.6832258491480455, Test RMSLE = 0.6897424863085482\n",
      "Epoch 118: Train RMSLE = 0.6831217335158835, Test RMSLE = 0.6896500217235003\n",
      "Epoch 119: Train RMSLE = 0.6831241397480877, Test RMSLE = 0.6895781387921879\n",
      "Epoch 120: Train RMSLE = 0.6831617165680909, Test RMSLE = 0.6895242182759532\n",
      "Epoch 121: Train RMSLE = 0.6831605565157894, Test RMSLE = 0.6896915985631314\n",
      "Epoch 122: Train RMSLE = 0.6831276986516067, Test RMSLE = 0.6897188799990303\n",
      "Epoch 123: Train RMSLE = 0.6831233287648, Test RMSLE = 0.6896444027597872\n",
      "Epoch 124: Train RMSLE = 0.6831589590334286, Test RMSLE = 0.6897193487404593\n",
      "Epoch 125: Train RMSLE = 0.6830219467832837, Test RMSLE = 0.6896713592725036\n",
      "Epoch 126: Train RMSLE = 0.6831296389934266, Test RMSLE = 0.6896360641845332\n",
      "Epoch 127: Train RMSLE = 0.6830558499123119, Test RMSLE = 0.6897285916884551\n",
      "Epoch 128: Train RMSLE = 0.683102897561515, Test RMSLE = 0.6896263358596076\n",
      "Epoch 129: Train RMSLE = 0.6830939770575174, Test RMSLE = 0.6897070741027922\n",
      "Epoch 130: Train RMSLE = 0.6830924903173103, Test RMSLE = 0.6897691381567113\n",
      "Epoch 131: Train RMSLE = 0.683158972699598, Test RMSLE = 0.6895893068633824\n",
      "Epoch 132: Train RMSLE = 0.6831525565915473, Test RMSLE = 0.6897231329403797\n",
      "Epoch 133: Train RMSLE = 0.6830603342760163, Test RMSLE = 0.6896619442870462\n",
      "Epoch 134: Train RMSLE = 0.6831636717990037, Test RMSLE = 0.6898181420877412\n",
      "Epoch 135: Train RMSLE = 0.6831408744689668, Test RMSLE = 0.6896752568061962\n",
      "Epoch 136: Train RMSLE = 0.6831335462048505, Test RMSLE = 0.6896426751582212\n",
      "Epoch 137: Train RMSLE = 0.6831486737806644, Test RMSLE = 0.6896568339577867\n",
      "Epoch 138: Train RMSLE = 0.6831672617567325, Test RMSLE = 0.6897462431709032\n",
      "Epoch 139: Train RMSLE = 0.6830185353530676, Test RMSLE = 0.6896786179560046\n",
      "Epoch 140: Train RMSLE = 0.6831368433979261, Test RMSLE = 0.6896407462317031\n",
      "Epoch 141: Train RMSLE = 0.683077295183022, Test RMSLE = 0.6896052801037577\n",
      "Epoch 142: Train RMSLE = 0.6830156789289541, Test RMSLE = 0.6897266685666649\n",
      "Epoch 143: Train RMSLE = 0.6831317863440026, Test RMSLE = 0.6897776053047572\n",
      "Epoch 144: Train RMSLE = 0.6830013492431015, Test RMSLE = 0.6897084105673785\n",
      "Epoch 145: Train RMSLE = 0.683119788665338, Test RMSLE = 0.6897700985238403\n",
      "Epoch 146: Train RMSLE = 0.6830761942976874, Test RMSLE = 0.6897057520534945\n",
      "Epoch 147: Train RMSLE = 0.6831023366821616, Test RMSLE = 0.6896877828812501\n",
      "Epoch 148: Train RMSLE = 0.6831446806620872, Test RMSLE = 0.6897410238968119\n",
      "Epoch 149: Train RMSLE = 0.6830603372126417, Test RMSLE = 0.6897985817572129\n",
      "Epoch 150: Train RMSLE = 0.6831517088047006, Test RMSLE = 0.6898000253904615\n",
      "Epoch 151: Train RMSLE = 0.683154495581715, Test RMSLE = 0.68972241805146\n",
      "Epoch 152: Train RMSLE = 0.6831242489326232, Test RMSLE = 0.6897150332277472\n",
      "Epoch 153: Train RMSLE = 0.6831487577874591, Test RMSLE = 0.6897342339997877\n",
      "Epoch 154: Train RMSLE = 0.6830472506847838, Test RMSLE = 0.689769363009474\n",
      "Epoch 155: Train RMSLE = 0.6830500926333456, Test RMSLE = 0.6896341281404662\n",
      "Epoch 156: Train RMSLE = 0.683022886488918, Test RMSLE = 0.6897009018365365\n",
      "Epoch 157: Train RMSLE = 0.6831779093118293, Test RMSLE = 0.6895810569936373\n",
      "Epoch 158: Train RMSLE = 0.6829820417825556, Test RMSLE = 0.6896901744355705\n",
      "Epoch 159: Train RMSLE = 0.682936581158109, Test RMSLE = 0.6897479342065448\n",
      "Epoch 160: Train RMSLE = 0.6830174317338555, Test RMSLE = 0.6896730316068822\n",
      "Epoch 161: Train RMSLE = 0.6831157254121015, Test RMSLE = 0.6896951184548716\n",
      "Epoch 162: Train RMSLE = 0.6830546230529904, Test RMSLE = 0.6897084676815248\n",
      "Epoch 163: Train RMSLE = 0.6831056385963655, Test RMSLE = 0.6897808387762937\n",
      "Epoch 164: Train RMSLE = 0.6831003931076401, Test RMSLE = 0.6897886151038037\n",
      "Epoch 165: Train RMSLE = 0.6830769104738356, Test RMSLE = 0.6897613146320014\n",
      "Epoch 166: Train RMSLE = 0.6830627730943029, Test RMSLE = 0.6896995396046215\n",
      "Epoch 167: Train RMSLE = 0.6830189268575246, Test RMSLE = 0.6897800305162025\n",
      "Epoch 168: Train RMSLE = 0.6829418085813587, Test RMSLE = 0.6897420312294664\n",
      "Epoch 169: Train RMSLE = 0.6830704315236111, Test RMSLE = 0.6896956487379998\n",
      "Epoch 170: Train RMSLE = 0.6830121388021372, Test RMSLE = 0.6896998845225816\n",
      "Epoch 171: Train RMSLE = 0.6830181070724063, Test RMSLE = 0.6896901582119352\n",
      "Epoch 172: Train RMSLE = 0.6830991761893989, Test RMSLE = 0.6896091773028097\n",
      "Epoch 173: Train RMSLE = 0.6830216255969244, Test RMSLE = 0.6897379848450468\n",
      "Epoch 174: Train RMSLE = 0.6830104899425099, Test RMSLE = 0.6897512136847328\n",
      "Epoch 175: Train RMSLE = 0.683057755944693, Test RMSLE = 0.6896367832307192\n",
      "Epoch 176: Train RMSLE = 0.6830291060554718, Test RMSLE = 0.6896942388262303\n",
      "Epoch 177: Train RMSLE = 0.6830316191518753, Test RMSLE = 0.689704359806889\n",
      "Epoch 178: Train RMSLE = 0.6830757462603763, Test RMSLE = 0.6897508704978944\n",
      "Epoch 179: Train RMSLE = 0.6830410728633651, Test RMSLE = 0.6896851966819195\n",
      "Epoch 180: Train RMSLE = 0.6830803030082754, Test RMSLE = 0.689807165855724\n",
      "Epoch 181: Train RMSLE = 0.6831157251610804, Test RMSLE = 0.6896812393183431\n",
      "Epoch 182: Train RMSLE = 0.6830104097348234, Test RMSLE = 0.689743810031049\n",
      "Epoch 183: Train RMSLE = 0.6831035065966969, Test RMSLE = 0.6896633562324012\n",
      "Epoch 184: Train RMSLE = 0.6830253350745009, Test RMSLE = 0.6896564378526807\n",
      "Epoch 185: Train RMSLE = 0.6830821092468855, Test RMSLE = 0.6896787087144051\n",
      "Epoch 186: Train RMSLE = 0.6829928026350726, Test RMSLE = 0.6896688611351421\n",
      "Epoch 187: Train RMSLE = 0.6830070965069686, Test RMSLE = 0.6896714880126157\n",
      "Epoch 188: Train RMSLE = 0.6829663139959178, Test RMSLE = 0.6896391392680847\n",
      "Epoch 189: Train RMSLE = 0.682965335034441, Test RMSLE = 0.6897709402930425\n",
      "Epoch 190: Train RMSLE = 0.6830584800714525, Test RMSLE = 0.6897793961147901\n",
      "Epoch 191: Train RMSLE = 0.6830511168944811, Test RMSLE = 0.6898372120410364\n",
      "Epoch 192: Train RMSLE = 0.6830317224937332, Test RMSLE = 0.6896958659326404\n",
      "Epoch 193: Train RMSLE = 0.6829815499130937, Test RMSLE = 0.6897585827932068\n",
      "Epoch 194: Train RMSLE = 0.683123982401287, Test RMSLE = 0.689900588993119\n",
      "Epoch 195: Train RMSLE = 0.682952196944269, Test RMSLE = 0.6898741182691978\n",
      "Epoch 196: Train RMSLE = 0.6830494824154679, Test RMSLE = 0.689787025374178\n",
      "Epoch 197: Train RMSLE = 0.683084774931952, Test RMSLE = 0.6898115088604512\n",
      "Epoch 198: Train RMSLE = 0.6829006743526994, Test RMSLE = 0.6897245835946462\n",
      "Epoch 199: Train RMSLE = 0.6829431962567216, Test RMSLE = 0.6897991405877612\n",
      "Epoch 200: Train RMSLE = 0.6829969567273838, Test RMSLE = 0.6898136088140735\n",
      "Epoch 201: Train RMSLE = 0.6829967086107, Test RMSLE = 0.6898083571120179\n",
      "Epoch 202: Train RMSLE = 0.6830053181220489, Test RMSLE = 0.6897740824656053\n",
      "Epoch 203: Train RMSLE = 0.6829639585971008, Test RMSLE = 0.6897935940093908\n",
      "Epoch 204: Train RMSLE = 0.6830880268881243, Test RMSLE = 0.689726506677824\n",
      "Epoch 205: Train RMSLE = 0.6829822473302399, Test RMSLE = 0.6896745844295076\n",
      "Epoch 206: Train RMSLE = 0.6829336058352641, Test RMSLE = 0.6898176067850035\n",
      "Epoch 207: Train RMSLE = 0.6829511246544246, Test RMSLE = 0.6897861258730315\n",
      "Epoch 208: Train RMSLE = 0.6829488467873446, Test RMSLE = 0.6897167035092342\n",
      "Epoch 209: Train RMSLE = 0.68295882607271, Test RMSLE = 0.6897406480632058\n",
      "Epoch 210: Train RMSLE = 0.6829847672267414, Test RMSLE = 0.6897585021383792\n",
      "Epoch 211: Train RMSLE = 0.6829487250935895, Test RMSLE = 0.6897313266870296\n",
      "Epoch 212: Train RMSLE = 0.6829613843384343, Test RMSLE = 0.6896810196138978\n",
      "Epoch 213: Train RMSLE = 0.6829185006486038, Test RMSLE = 0.6897947147594778\n",
      "Epoch 214: Train RMSLE = 0.6828773349924213, Test RMSLE = 0.6898066321875771\n",
      "Epoch 215: Train RMSLE = 0.6829052913263597, Test RMSLE = 0.6898419356608695\n",
      "Epoch 216: Train RMSLE = 0.6829422008131333, Test RMSLE = 0.6897248464226852\n",
      "Epoch 217: Train RMSLE = 0.6829303495296681, Test RMSLE = 0.6897856361264674\n",
      "Epoch 218: Train RMSLE = 0.6828585583707579, Test RMSLE = 0.6898745700662271\n",
      "Epoch 219: Train RMSLE = 0.682951584435019, Test RMSLE = 0.689765659509429\n",
      "Epoch 220: Train RMSLE = 0.6830046415042558, Test RMSLE = 0.6896379029318366\n",
      "Epoch 221: Train RMSLE = 0.6829079710702184, Test RMSLE = 0.6897109961102987\n",
      "Epoch 222: Train RMSLE = 0.6829279902319781, Test RMSLE = 0.6898133947470357\n",
      "Epoch 223: Train RMSLE = 0.6829147600460166, Test RMSLE = 0.6897506371053602\n",
      "Epoch 224: Train RMSLE = 0.6830173627352278, Test RMSLE = 0.6899016636850714\n",
      "Epoch 225: Train RMSLE = 0.6829164191605006, Test RMSLE = 0.6898894963769515\n",
      "Epoch 226: Train RMSLE = 0.6829603552483603, Test RMSLE = 0.6897549603920317\n",
      "Epoch 227: Train RMSLE = 0.6829390603425236, Test RMSLE = 0.689809494632749\n",
      "Epoch 228: Train RMSLE = 0.6829799099614982, Test RMSLE = 0.6897877202297076\n",
      "Epoch 229: Train RMSLE = 0.682862042334816, Test RMSLE = 0.6897634227585777\n",
      "Epoch 230: Train RMSLE = 0.6829326197486493, Test RMSLE = 0.6898679184658794\n",
      "Epoch 231: Train RMSLE = 0.6829231244448896, Test RMSLE = 0.6897680185585603\n",
      "Epoch 232: Train RMSLE = 0.6830161420790245, Test RMSLE = 0.6898509451841541\n",
      "Epoch 233: Train RMSLE = 0.6829408380049531, Test RMSLE = 0.6899470387988357\n",
      "Epoch 234: Train RMSLE = 0.6829477059397685, Test RMSLE = 0.6897893127844725\n",
      "Epoch 235: Train RMSLE = 0.6829009973299979, Test RMSLE = 0.6897633443690098\n",
      "Epoch 236: Train RMSLE = 0.6828841910455701, Test RMSLE = 0.6898152757813312\n",
      "Epoch 237: Train RMSLE = 0.6828968235791923, Test RMSLE = 0.6897474882721618\n",
      "Epoch 238: Train RMSLE = 0.6828545761506813, Test RMSLE = 0.6898183728546335\n",
      "Epoch 239: Train RMSLE = 0.6828926728016472, Test RMSLE = 0.6897775197075449\n",
      "Epoch 240: Train RMSLE = 0.682973703068293, Test RMSLE = 0.6898330134144357\n",
      "Epoch 241: Train RMSLE = 0.6829145067946635, Test RMSLE = 0.6898013781880848\n",
      "Epoch 242: Train RMSLE = 0.6829157342268282, Test RMSLE = 0.6898694079127174\n",
      "Epoch 243: Train RMSLE = 0.6829138820481335, Test RMSLE = 0.6898991906908052\n",
      "Epoch 244: Train RMSLE = 0.6829097754438703, Test RMSLE = 0.6897558113638512\n",
      "Epoch 245: Train RMSLE = 0.6829858147668681, Test RMSLE = 0.6898264919313749\n",
      "Epoch 246: Train RMSLE = 0.6829382678430801, Test RMSLE = 0.6897905650740693\n",
      "Epoch 247: Train RMSLE = 0.6829921953152016, Test RMSLE = 0.6899091289190669\n",
      "Epoch 248: Train RMSLE = 0.6829624847394268, Test RMSLE = 0.6896881152823635\n",
      "Epoch 249: Train RMSLE = 0.6829319009528704, Test RMSLE = 0.6898003237032783\n",
      "Epoch 250: Train RMSLE = 0.6828980287444958, Test RMSLE = 0.6897914836111194\n",
      "Epoch 251: Train RMSLE = 0.6830720074728649, Test RMSLE = 0.689944293123938\n",
      "Epoch 252: Train RMSLE = 0.6829133029375662, Test RMSLE = 0.6899180021470835\n",
      "Epoch 253: Train RMSLE = 0.6827812525496281, Test RMSLE = 0.6899148381585257\n",
      "Epoch 254: Train RMSLE = 0.6829211539773162, Test RMSLE = 0.6897736357139269\n",
      "Epoch 255: Train RMSLE = 0.6828081046979478, Test RMSLE = 0.6897997724858043\n",
      "Epoch 256: Train RMSLE = 0.6829779527047162, Test RMSLE = 0.6897737495818372\n",
      "Epoch 257: Train RMSLE = 0.6829739512927872, Test RMSLE = 0.6897509239052522\n",
      "Epoch 258: Train RMSLE = 0.6827981805519475, Test RMSLE = 0.6899819680731129\n",
      "Epoch 259: Train RMSLE = 0.6829277379090813, Test RMSLE = 0.6897782369646905\n",
      "Epoch 260: Train RMSLE = 0.6827911791865302, Test RMSLE = 0.6898331547035714\n",
      "Epoch 261: Train RMSLE = 0.6829346485561071, Test RMSLE = 0.68980779966508\n",
      "Epoch 262: Train RMSLE = 0.6828588390300782, Test RMSLE = 0.6898308826674397\n",
      "Epoch 263: Train RMSLE = 0.6829284632008364, Test RMSLE = 0.6898668325248534\n",
      "Epoch 264: Train RMSLE = 0.6828384847631594, Test RMSLE = 0.6898157481716303\n",
      "Epoch 265: Train RMSLE = 0.6829229175809534, Test RMSLE = 0.6898534341188981\n",
      "Epoch 266: Train RMSLE = 0.6827628190657997, Test RMSLE = 0.6899043632117391\n",
      "Epoch 267: Train RMSLE = 0.6829429914685371, Test RMSLE = 0.6898859125044968\n",
      "Epoch 268: Train RMSLE = 0.6828703301818999, Test RMSLE = 0.6897931696207161\n",
      "Epoch 269: Train RMSLE = 0.682830729421996, Test RMSLE = 0.6898631592518669\n",
      "Epoch 270: Train RMSLE = 0.6828017811812925, Test RMSLE = 0.6898361782653366\n",
      "Epoch 271: Train RMSLE = 0.6828828637182607, Test RMSLE = 0.6897855294533307\n",
      "Epoch 272: Train RMSLE = 0.6828722320724323, Test RMSLE = 0.6898680392775827\n",
      "Epoch 273: Train RMSLE = 0.6827931863739889, Test RMSLE = 0.689884412503608\n",
      "Epoch 274: Train RMSLE = 0.6827924638434663, Test RMSLE = 0.6898919823256715\n",
      "Epoch 275: Train RMSLE = 0.6827671026286937, Test RMSLE = 0.6899346289470474\n",
      "Epoch 276: Train RMSLE = 0.6830153076091703, Test RMSLE = 0.6898578708101313\n",
      "Epoch 277: Train RMSLE = 0.6828486979278051, Test RMSLE = 0.689812339708785\n",
      "Epoch 278: Train RMSLE = 0.6829067604355508, Test RMSLE = 0.6897844810757157\n",
      "Epoch 279: Train RMSLE = 0.6828841000954666, Test RMSLE = 0.6898243334864314\n",
      "Epoch 280: Train RMSLE = 0.6828852505092003, Test RMSLE = 0.6898028446476857\n",
      "Epoch 281: Train RMSLE = 0.6828586291860707, Test RMSLE = 0.6897756486802914\n",
      "Epoch 282: Train RMSLE = 0.6828363668898657, Test RMSLE = 0.6899000027027328\n",
      "Epoch 283: Train RMSLE = 0.6828851643220619, Test RMSLE = 0.6898164222408283\n",
      "Epoch 284: Train RMSLE = 0.6829493535281991, Test RMSLE = 0.6899222544577641\n",
      "Epoch 285: Train RMSLE = 0.6828019762295355, Test RMSLE = 0.6898767829739334\n",
      "Epoch 286: Train RMSLE = 0.6828675178908405, Test RMSLE = 0.6899242070794132\n",
      "Epoch 287: Train RMSLE = 0.6828331865036785, Test RMSLE = 0.6898345687275595\n",
      "Epoch 288: Train RMSLE = 0.6828473917216707, Test RMSLE = 0.6898395374008657\n",
      "Epoch 289: Train RMSLE = 0.6829169364329787, Test RMSLE = 0.6898671199103664\n",
      "Epoch 290: Train RMSLE = 0.6827825812945632, Test RMSLE = 0.689951810581178\n",
      "Epoch 291: Train RMSLE = 0.6827946995019875, Test RMSLE = 0.689861023800596\n",
      "Epoch 292: Train RMSLE = 0.6828633261324873, Test RMSLE = 0.6899608792457589\n",
      "Epoch 293: Train RMSLE = 0.6828244552792654, Test RMSLE = 0.6899011130660895\n",
      "Epoch 294: Train RMSLE = 0.6827393534258036, Test RMSLE = 0.689869244878376\n",
      "Epoch 295: Train RMSLE = 0.6828430699569014, Test RMSLE = 0.6898439896078404\n",
      "Epoch 296: Train RMSLE = 0.6827628416737923, Test RMSLE = 0.6898072667176974\n",
      "Epoch 297: Train RMSLE = 0.6828458233658061, Test RMSLE = 0.6898857006962833\n",
      "Epoch 298: Train RMSLE = 0.6827691701609877, Test RMSLE = 0.6899556597786057\n",
      "Epoch 299: Train RMSLE = 0.682814459944639, Test RMSLE = 0.6898897661938315\n",
      "Epoch 300: Train RMSLE = 0.6828113386159993, Test RMSLE = 0.6898105782827737\n",
      "Epoch 301: Train RMSLE = 0.68278587718426, Test RMSLE = 0.6897869658853704\n",
      "Epoch 302: Train RMSLE = 0.6829184341344415, Test RMSLE = 0.6898154702525858\n",
      "Epoch 303: Train RMSLE = 0.6827250107914445, Test RMSLE = 0.6899311863379869\n",
      "Epoch 304: Train RMSLE = 0.6828243556464055, Test RMSLE = 0.6898257146834607\n",
      "Epoch 305: Train RMSLE = 0.6828266569115509, Test RMSLE = 0.6899588460207655\n",
      "Epoch 306: Train RMSLE = 0.6827520115619717, Test RMSLE = 0.689956590272623\n",
      "Epoch 307: Train RMSLE = 0.6827287795357528, Test RMSLE = 0.6898291306628493\n",
      "Epoch 308: Train RMSLE = 0.6828678429599818, Test RMSLE = 0.6898724980679682\n",
      "Epoch 309: Train RMSLE = 0.6828177922645814, Test RMSLE = 0.6898675188148699\n",
      "Epoch 310: Train RMSLE = 0.6828236725117178, Test RMSLE = 0.6899505018338455\n",
      "Epoch 311: Train RMSLE = 0.6828478116880617, Test RMSLE = 0.6899587543292325\n",
      "Epoch 312: Train RMSLE = 0.682805540621728, Test RMSLE = 0.6899096096432047\n",
      "Epoch 313: Train RMSLE = 0.6827839765214784, Test RMSLE = 0.6898737713691355\n",
      "Epoch 314: Train RMSLE = 0.6827877878203041, Test RMSLE = 0.6898254746753385\n",
      "Epoch 315: Train RMSLE = 0.6827460456152571, Test RMSLE = 0.6898702461102745\n",
      "Epoch 316: Train RMSLE = 0.6827071906117665, Test RMSLE = 0.689917477424138\n",
      "Epoch 317: Train RMSLE = 0.682823524909686, Test RMSLE = 0.6898280373983419\n",
      "Epoch 318: Train RMSLE = 0.6827902201587055, Test RMSLE = 0.6900396254569247\n",
      "Epoch 319: Train RMSLE = 0.6827753938742175, Test RMSLE = 0.6898978784554877\n",
      "Epoch 320: Train RMSLE = 0.6828010285201891, Test RMSLE = 0.6898172511521078\n",
      "Epoch 321: Train RMSLE = 0.6827276846845801, Test RMSLE = 0.6898789645412575\n",
      "Epoch 322: Train RMSLE = 0.6828190571149851, Test RMSLE = 0.6900379012789898\n",
      "Epoch 323: Train RMSLE = 0.6827516490327546, Test RMSLE = 0.6899207171573157\n",
      "Epoch 324: Train RMSLE = 0.6827062486823424, Test RMSLE = 0.6898958312335063\n",
      "Epoch 325: Train RMSLE = 0.6826958955331907, Test RMSLE = 0.6897936899289795\n",
      "Epoch 326: Train RMSLE = 0.6827247646845389, Test RMSLE = 0.6898488320765808\n",
      "Epoch 327: Train RMSLE = 0.682770612217825, Test RMSLE = 0.68987585732577\n",
      "Epoch 328: Train RMSLE = 0.6827548929546418, Test RMSLE = 0.6899154325575119\n",
      "Epoch 329: Train RMSLE = 0.6827840387393175, Test RMSLE = 0.6897805454444741\n",
      "Epoch 330: Train RMSLE = 0.6828394307266217, Test RMSLE = 0.6899284899646637\n",
      "Epoch 331: Train RMSLE = 0.6827973126512284, Test RMSLE = 0.6899150334599465\n",
      "Epoch 332: Train RMSLE = 0.6827931127813528, Test RMSLE = 0.6900890320442944\n",
      "Epoch 333: Train RMSLE = 0.6827868238251575, Test RMSLE = 0.6899946962968496\n",
      "Epoch 334: Train RMSLE = 0.6828346780245508, Test RMSLE = 0.6899292569996033\n",
      "Epoch 335: Train RMSLE = 0.6827300888056322, Test RMSLE = 0.6898994563439834\n",
      "Epoch 336: Train RMSLE = 0.6828056505319721, Test RMSLE = 0.6899041459398737\n",
      "Epoch 337: Train RMSLE = 0.6827451376396587, Test RMSLE = 0.690031388149074\n",
      "Epoch 338: Train RMSLE = 0.6827208071305063, Test RMSLE = 0.6898662796856595\n",
      "Epoch 339: Train RMSLE = 0.6827605676959306, Test RMSLE = 0.6898669205739518\n",
      "Epoch 340: Train RMSLE = 0.6827852569239281, Test RMSLE = 0.689945172894158\n",
      "Epoch 341: Train RMSLE = 0.6826504739167836, Test RMSLE = 0.6898358280574804\n",
      "Epoch 342: Train RMSLE = 0.6827975071475467, Test RMSLE = 0.6900678303207751\n",
      "Epoch 343: Train RMSLE = 0.6827105243058887, Test RMSLE = 0.6899777106141148\n",
      "Epoch 344: Train RMSLE = 0.6826898596239916, Test RMSLE = 0.6900283146872425\n",
      "Epoch 345: Train RMSLE = 0.6827992750265887, Test RMSLE = 0.6899345801860461\n",
      "Epoch 346: Train RMSLE = 0.682763860809913, Test RMSLE = 0.6899536747096863\n",
      "Epoch 347: Train RMSLE = 0.6827295992774252, Test RMSLE = 0.6899529025779123\n",
      "Epoch 348: Train RMSLE = 0.6827587036259147, Test RMSLE = 0.6899904666902488\n",
      "Epoch 349: Train RMSLE = 0.6827735899221523, Test RMSLE = 0.6898625829556518\n",
      "Epoch 350: Train RMSLE = 0.6826572029065779, Test RMSLE = 0.6900037656371472\n",
      "Epoch 351: Train RMSLE = 0.6826848178035104, Test RMSLE = 0.6899735136864588\n",
      "Epoch 352: Train RMSLE = 0.6827202261630264, Test RMSLE = 0.6898958408994722\n",
      "Epoch 353: Train RMSLE = 0.6826998021025797, Test RMSLE = 0.6899281287844561\n",
      "Epoch 354: Train RMSLE = 0.6826551802925837, Test RMSLE = 0.6899946803821134\n",
      "Epoch 355: Train RMSLE = 0.6826874282301376, Test RMSLE = 0.6899250939285696\n",
      "Epoch 356: Train RMSLE = 0.6826401299910491, Test RMSLE = 0.6899630645520485\n",
      "Epoch 357: Train RMSLE = 0.6827245556354317, Test RMSLE = 0.6899229754924877\n",
      "Epoch 358: Train RMSLE = 0.6827350928249064, Test RMSLE = 0.6899048475783115\n",
      "Epoch 359: Train RMSLE = 0.6826589671506409, Test RMSLE = 0.689947937501993\n",
      "Epoch 360: Train RMSLE = 0.6827699314919746, Test RMSLE = 0.6899752761292628\n",
      "Epoch 361: Train RMSLE = 0.6827195251929657, Test RMSLE = 0.6900446036095702\n",
      "Epoch 362: Train RMSLE = 0.6827314245083582, Test RMSLE = 0.6899733698102934\n",
      "Epoch 363: Train RMSLE = 0.682734936625401, Test RMSLE = 0.689989132246377\n",
      "Epoch 364: Train RMSLE = 0.6827870759195607, Test RMSLE = 0.6899442446847066\n",
      "Epoch 365: Train RMSLE = 0.6827335152359698, Test RMSLE = 0.6900037214002298\n",
      "Epoch 366: Train RMSLE = 0.6827318989479428, Test RMSLE = 0.6898588919595827\n",
      "Epoch 367: Train RMSLE = 0.6825830876894823, Test RMSLE = 0.6899017546751463\n",
      "Epoch 368: Train RMSLE = 0.6827623794232143, Test RMSLE = 0.6899815273127888\n",
      "Epoch 369: Train RMSLE = 0.6826749013377912, Test RMSLE = 0.6898695525353888\n",
      "Epoch 370: Train RMSLE = 0.6825288754773381, Test RMSLE = 0.6899424590422972\n",
      "Epoch 371: Train RMSLE = 0.6826472193297168, Test RMSLE = 0.6900871904525456\n",
      "Epoch 372: Train RMSLE = 0.6825850461225235, Test RMSLE = 0.6899585443808486\n",
      "Epoch 373: Train RMSLE = 0.682647530240301, Test RMSLE = 0.6901484299069011\n",
      "Epoch 374: Train RMSLE = 0.6826536096437376, Test RMSLE = 0.6899888432906272\n",
      "Epoch 375: Train RMSLE = 0.682643583996701, Test RMSLE = 0.6900405689311373\n",
      "Epoch 376: Train RMSLE = 0.6826684208750036, Test RMSLE = 0.6899320574976461\n",
      "Epoch 377: Train RMSLE = 0.6827535734089872, Test RMSLE = 0.6899109733359547\n",
      "Epoch 378: Train RMSLE = 0.6826559380593925, Test RMSLE = 0.690094094899632\n",
      "Epoch 379: Train RMSLE = 0.6825863021936243, Test RMSLE = 0.6900287882873961\n",
      "Epoch 380: Train RMSLE = 0.6826259123636021, Test RMSLE = 0.6900046744760543\n",
      "Epoch 381: Train RMSLE = 0.6827397927900131, Test RMSLE = 0.6899394540209102\n",
      "Epoch 382: Train RMSLE = 0.682810542703634, Test RMSLE = 0.6900338040935481\n",
      "Epoch 383: Train RMSLE = 0.6825911605305345, Test RMSLE = 0.690078363810366\n",
      "Epoch 384: Train RMSLE = 0.682626741272373, Test RMSLE = 0.6898535036791017\n",
      "Epoch 385: Train RMSLE = 0.6826622000774808, Test RMSLE = 0.6899917628306391\n",
      "Epoch 386: Train RMSLE = 0.6826239003021499, Test RMSLE = 0.6900808231808974\n",
      "Epoch 387: Train RMSLE = 0.6826691553579938, Test RMSLE = 0.6900337129490238\n",
      "Epoch 388: Train RMSLE = 0.6826331723403174, Test RMSLE = 0.6899581033631088\n",
      "Epoch 389: Train RMSLE = 0.682568183517644, Test RMSLE = 0.6899581658829883\n",
      "Epoch 390: Train RMSLE = 0.682673949015127, Test RMSLE = 0.6900665776193129\n",
      "Epoch 391: Train RMSLE = 0.6826296321378723, Test RMSLE = 0.6898878727649618\n",
      "Epoch 392: Train RMSLE = 0.6826315672710038, Test RMSLE = 0.6900259180489587\n",
      "Epoch 393: Train RMSLE = 0.6826449977283787, Test RMSLE = 0.6900881615539164\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m AdvancedMLP(num_features)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Minimum Train RMSLE so far: 0.6882604910622443\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Minimum Test RMSLE so far: 0.687623768491432\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 35\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, test_loader, num_epochs)\u001b[0m\n\u001b[0;32m     33\u001b[0m     loss \u001b[38;5;241m=\u001b[39m rmsle_loss(outputs, targets)\n\u001b[0;32m     34\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 35\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     39\u001b[0m avg_train_rmsle \u001b[38;5;241m=\u001b[39m total_train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "File \u001b[1;32mc:\\Users\\divya\\OneDrive\\Documents\\UTAUSTIN\\Semester 6\\461p\\ECE-461P-Term-Project\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\divya\\OneDrive\\Documents\\UTAUSTIN\\Semester 6\\461p\\ECE-461P-Term-Project\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\divya\\OneDrive\\Documents\\UTAUSTIN\\Semester 6\\461p\\ECE-461P-Term-Project\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    158\u001b[0m         group,\n\u001b[0;32m    159\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    164\u001b[0m         state_steps)\n\u001b[1;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\divya\\OneDrive\\Documents\\UTAUSTIN\\Semester 6\\461p\\ECE-461P-Term-Project\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\divya\\OneDrive\\Documents\\UTAUSTIN\\Semester 6\\461p\\ECE-461P-Term-Project\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:508\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[38;5;66;03m# Update steps\u001b[39;00m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m# If steps are on CPU, foreach will fall back to the slow path, which is a for-loop calling t.add(1) over\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# and over. 1 will then be wrapped into a Tensor over and over again, which is slower than if we just\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# wrapped it once now. The alpha is required to assure we go to the right overload.\u001b[39;00m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_state_steps[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mis_cpu:\n\u001b[1;32m--> 508\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_add_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_state_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_add_(device_state_steps, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = AdvancedMLP(num_features).to(device)\n",
    "train_model(model, train_loader, test_loader, num_epochs=1000).to(device)\n",
    "# Minimum Train RMSLE so far: 0.6882604910622443\n",
    "# Minimum Test RMSLE so far: 0.687623768491432"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomMLP(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(CustomMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_features, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        \n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc5 = nn.Linear(128, 64)\n",
    "        self.fc6 = nn.Linear(64, 32)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        self.final_layer = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.relu(self.fc6(x))\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.final_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model10 = CustomMLP(num_features).to(device)\n",
    "train_model(model10, train_loader, test_loader, num_epochs=100).to(device)\n",
    "\n",
    "# Minimum Train RMSLE so far: 0.6903926233359389\n",
    "# Minimum Test RMSLE so far: 0.6875400086750199"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Using cached xgboost-2.0.3-py3-none-win_amd64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from xgboost) (1.13.0)\n",
      "Using cached xgboost-2.0.3-py3-none-win_amd64.whl (99.8 MB)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Assume `train` is your DataFrame loaded with the data\n",
    "X = train.drop(columns=['price'])\n",
    "y = np.log1p(train['price'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    # prevent negative predictions\n",
    "    preds = np.clip(preds, a_min=0, a_max=None)\n",
    "    return 'RMSLE', np.sqrt(np.mean(np.power((preds-labels), 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 26\u001b[0m\n\u001b[0;32m     22\u001b[0m random_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(estimator\u001b[38;5;241m=\u001b[39mxgb_model, param_distributions\u001b[38;5;241m=\u001b[39mparam_dist, \n\u001b[0;32m     23\u001b[0m                                    n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, scoring\u001b[38;5;241m=\u001b[39mrmsle_scoring, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[43mrandom_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\divya\\OneDrive\\Documents\\UTAUSTIN\\Semester 6\\461p\\ECE-461P-Term-Project\\.venv\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\divya\\OneDrive\\Documents\\UTAUSTIN\\Semester 6\\461p\\ECE-461P-Term-Project\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    966\u001b[0m     )\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\divya\\OneDrive\\Documents\\UTAUSTIN\\Semester 6\\461p\\ECE-461P-Term-Project\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1914\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1913\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1914\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1915\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1916\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[0;32m   1917\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1918\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\divya\\OneDrive\\Documents\\UTAUSTIN\\Semester 6\\461p\\ECE-461P-Term-Project\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    913\u001b[0m         )\n\u001b[0;32m    914\u001b[0m     )\n\u001b[1;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    939\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\divya\\OneDrive\\Documents\\UTAUSTIN\\Semester 6\\461p\\ECE-461P-Term-Project\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\divya\\OneDrive\\Documents\\UTAUSTIN\\Semester 6\\461p\\ECE-461P-Term-Project\\.venv\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\divya\\OneDrive\\Documents\\UTAUSTIN\\Semester 6\\461p\\ECE-461P-Term-Project\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\divya\\OneDrive\\Documents\\UTAUSTIN\\Semester 6\\461p\\ECE-461P-Term-Project\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scipy.stats import randint, uniform\n",
    "\n",
    "param_dist = {\n",
    "    'max_depth': randint(0, 100),\n",
    "    'min_child_weight': randint(0, 100),\n",
    "    'subsample': uniform(0.6, 0.6),\n",
    "    'colsample_bytree': uniform(0.6, 0.6),\n",
    "    'learning_rate': uniform(0.01, 0.8),\n",
    "    'n_estimators': randint(0,1000)\n",
    "}\n",
    "\n",
    "# Create the RMSLE scorer\n",
    "def rmsle_scorer(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.power(y_pred - y_true, 2)))\n",
    "\n",
    "rmsle_scoring = make_scorer(rmsle_scorer, greater_is_better=False)\n",
    "\n",
    "# Initialize the XGBoost Regressor\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_jobs=10)\n",
    "\n",
    "# Setup RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_dist, \n",
    "                                   n_iter=100, scoring=rmsle_scoring, cv=3, verbose=3, random_state=42,n_jobs=10)\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model parameters: {'colsample_bytree': 0.8253152871382157, 'learning_rate': 0.14562495076197482, 'max_depth': 4, 'min_child_weight': 5, 'n_estimators': 200, 'subsample': 0.8736932106048627}\n",
      "Test RMSLE: 0.7570922261734949\n"
     ]
    }
   ],
   "source": [
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_rmsle = rmsle_scorer(y_test, y_pred)\n",
    "\n",
    "print(\"Best model parameters:\", random_search.best_params_)\n",
    "print(\"Test RMSLE:\", test_rmsle)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
