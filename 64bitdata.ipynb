{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "# from transformers import BertTokenizer, TFBertModel\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "import torch.nn as nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train=pd.read_csv('C:/Users/divya/OneDrive/Documents/UTAUSTIN/Semester 6/461p/ECE-461P-Term-Project/train_pp32.csv' ,delimiter=',')\n",
    "# test=pd.read_csv('C:/Users/divya/OneDrive/Documents/UTAUSTIN/Semester 6/461p/ECE-461P-Term-Project/test_pp32.csv' ,delimiter=',')\n",
    "\n",
    "train=pd.read_csv('C:/Users/divya/OneDrive/Documents/UTAUSTIN/Semester 6/461p/ECE-461P-Term-Project/train_pp2.csv' ,delimiter=',')\n",
    "test=pd.read_csv('C:/Users/divya/OneDrive/Documents/UTAUSTIN/Semester 6/461p/ECE-461P-Term-Project/test_pp2.csv' ,delimiter=',')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = train.drop(columns=['price'])\n",
    "y = ((train['price'])) # talk about this \n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train.astype(np.float32)).to(device)\n",
    "X_test_tensor = torch.tensor(X_test.astype(np.float32)).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values.astype(np.float32)).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.values.astype(np.float32)).to(device)\n",
    "\n",
    "# # Reshape y tensors to have the correct shape (n_samples, 1)\n",
    "y_train_tensor = y_train_tensor.view(-1, 1)\n",
    "y_test_tensor = y_test_tensor.view(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def rmsle_loss(y_pred, y_true):\n",
    "    # Ensure the inputs are float tensors (required for MSELoss and logarithmic operations)\n",
    "\n",
    "    \n",
    "    # Compute the RMSLE using the logarithmic differences\n",
    "    return torch.sqrt(torch.mean((torch.log1p(y_pred) - torch.log1p(y_true)) ** 2))\n",
    "    # return torch.sqrt(torch.mean((y_pred - y_true) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "\n",
    "def train_model(model, train_loader, test_loader, num_epochs=5):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-6, momentum=0.8)\n",
    "    scheduler = CyclicLR(optimizer, base_lr=1e-6, max_lr=0.1, step_size_up=5*len(train_loader),\n",
    "                     mode='triangular', cycle_momentum=False)\n",
    "\n",
    "\n",
    "    min_train_rmsle = float('inf')\n",
    "    min_test_rmsle = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = rmsle_loss(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update learning rate at each batch\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_rmsle = total_train_loss / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                test_loss = rmsle_loss(outputs, targets)\n",
    "                total_test_loss += test_loss.item()\n",
    "\n",
    "        avg_test_rmsle = total_test_loss / len(test_loader)\n",
    "\n",
    "        if avg_train_rmsle < min_train_rmsle:\n",
    "            min_train_rmsle = avg_train_rmsle\n",
    "        if avg_test_rmsle < min_test_rmsle:\n",
    "            min_test_rmsle = avg_test_rmsle\n",
    "\n",
    "        print(f'Epoch {epoch+1}: Train RMSLE = {avg_train_rmsle}, Test RMSLE = {avg_test_rmsle} , Current LR = {scheduler.get_last_lr()}')\n",
    "\n",
    "    print(f'Minimum Train RMSLE so far: {min_train_rmsle}')\n",
    "    print(f'Minimum Test RMSLE so far: {min_test_rmsle}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(model, train_loader, test_loader, num_epochs=5):\n",
    "#     model.to(device)\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "#     scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbose=True, min_lr=1e-6)\n",
    "\n",
    "#     # Initialize minimum loss to a large value\n",
    "#     min_train_rmsle = float('inf')\n",
    "#     min_test_rmsle = float('inf')\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         total_train_loss = 0\n",
    "#         for inputs, targets in train_loader:\n",
    "#             inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = rmsle_loss(outputs, targets)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             total_train_loss += loss.item()\n",
    "\n",
    "#         avg_train_rmsle = total_train_loss / len(train_loader)\n",
    "\n",
    "#         model.eval()\n",
    "#         total_test_loss = 0\n",
    "#         with torch.no_grad():\n",
    "#             for inputs, targets in test_loader:\n",
    "#                 inputs, targets = inputs.to(device), targets.to(device)\n",
    "#                 outputs = model(inputs)\n",
    "#                 test_loss = rmsle_loss(outputs, targets)\n",
    "#                 total_test_loss += test_loss.item()\n",
    "\n",
    "#         avg_test_rmsle = total_test_loss / len(test_loader)\n",
    "        \n",
    "#         # Update the learning rate scheduler\n",
    "#         scheduler.step(avg_test_rmsle)\n",
    "\n",
    "#         if avg_train_rmsle < min_train_rmsle:\n",
    "#             min_train_rmsle = avg_train_rmsle\n",
    "#         if avg_test_rmsle < min_test_rmsle:\n",
    "#             min_test_rmsle = avg_test_rmsle\n",
    "\n",
    "#         print(f'Epoch {epoch+1}: Train RMSLE = {avg_train_rmsle}, Test RMSLE = {avg_test_rmsle} , {scheduler.get_last_lr()}')\n",
    "    \n",
    "#     print(f'Minimum Train RMSLE so far: {min_train_rmsle}')\n",
    "#     print(f'Minimum Test RMSLE so far: {min_test_rmsle}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X_train.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AdvancedMLP(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(AdvancedMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_features, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.dropout3 = nn.Dropout(0.1)\n",
    "        \n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.final_layer = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.leaky_relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.leaky_relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        x = F.leaky_relu(self.bn4(self.fc4(x)))\n",
    "        x = self.final_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train RMSLE = 0.7514917956175907, Test RMSLE = 0.691889699742976 , Current LR = [0.020080000000000018]\n",
      "Epoch 2: Train RMSLE = 0.6929407155532386, Test RMSLE = 0.6899763355093947 , Current LR = [0.04006]\n",
      "Epoch 3: Train RMSLE = 0.6914681243433587, Test RMSLE = 0.6898108521099869 , Current LR = [0.06004000000000001]\n",
      "Epoch 4: Train RMSLE = 0.6908821830577122, Test RMSLE = 0.6894941653712208 , Current LR = [0.08002]\n",
      "Epoch 5: Train RMSLE = 0.6907152118919099, Test RMSLE = 0.6900911300057867 , Current LR = [0.1]\n",
      "Epoch 6: Train RMSLE = 0.6903489363416979, Test RMSLE = 0.6894711216729648 , Current LR = [0.08002]\n",
      "Epoch 7: Train RMSLE = 0.689899885309937, Test RMSLE = 0.6891402827807699 , Current LR = [0.06004000000000001]\n",
      "Epoch 8: Train RMSLE = 0.6895308730073392, Test RMSLE = 0.6893058754753077 , Current LR = [0.04006]\n",
      "Epoch 9: Train RMSLE = 0.6890082106383297, Test RMSLE = 0.6892872435780095 , Current LR = [0.020080000000000018]\n",
      "Epoch 10: Train RMSLE = 0.688635042431471, Test RMSLE = 0.6890960018159236 , Current LR = [0.0001]\n",
      "Minimum Train RMSLE so far: 0.688635042431471\n",
      "Minimum Test RMSLE so far: 0.6890960018159236\n"
     ]
    }
   ],
   "source": [
    "model = AdvancedMLP(num_features).to(device)\n",
    "train_model(model, train_loader, test_loader, num_epochs=10)\n",
    "# Minimum Train RMSLE so far: 0.6773702232499507\n",
    "# Minimum Test RMSLE so far: 0.678769255297098"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomMLP(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(CustomMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_features, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        \n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc5 = nn.Linear(128, 64)\n",
    "        self.fc6 = nn.Linear(64, 32)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        self.final_layer = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.relu(self.fc6(x))\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.final_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model10 = CustomMLP(num_features).to(device)\n",
    "train_model(model10, train_loader, test_loader, num_epochs=10)\n",
    "\n",
    "# Minimum Train RMSLE so far: 0.6754996292528404\n",
    "# Minimum Test RMSLE so far: 0.6798468978210392"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DynamicMLP(nn.Module):\n",
    "    def __init__(self, num_features, max_first_layer_size, min_layer_size=32, output_size=1):\n",
    "        super(DynamicMLP, self).__init__()\n",
    "        layers = []\n",
    "        current_size = max_first_layer_size\n",
    "        # Ensure the loop creates layers correctly by adjusting input and output sizes\n",
    "        input_size = num_features  # Starting number of features\n",
    "        while current_size > min_layer_size:\n",
    "            next_size = max(current_size // 2, min_layer_size)\n",
    "            layers.append(nn.Linear(input_size, current_size))\n",
    "            layers.append(nn.BatchNorm1d(current_size))\n",
    "            # layers.append(nn.Dropout(0.05))  # Using a constant dropout rate\n",
    "            layers.append(nn.ReLU())\n",
    "            input_size = current_size  # Update input size for the next layer\n",
    "            current_size = next_size\n",
    "        \n",
    "        # Append the final layer\n",
    "        layers.append(nn.Linear(input_size, output_size))\n",
    "\n",
    "        # Register all layers\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Example of using the DynamicMLP class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DynamicMLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=48, out_features=1024, bias=True)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (7): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU()\n",
      "    (9): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (10): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (13): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU()\n",
      "    (15): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "max_first_layer_size = 1024  # Adjusted for demonstration\n",
    "model = DynamicMLP(num_features, max_first_layer_size).to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train RMSLE = 0.7755220616465888, Test RMSLE = 0.6898555177126824 , Current LR = [0.02000080000000002]\n",
      "Epoch 2: Train RMSLE = 0.6901063022337403, Test RMSLE = 0.6879812699859111 , Current LR = [0.0400006]\n",
      "Epoch 3: Train RMSLE = 0.6891731141734524, Test RMSLE = 0.6884494067505054 , Current LR = [0.06000040000000001]\n",
      "Epoch 4: Train RMSLE = 0.6888162409863118, Test RMSLE = 0.6885505358649272 , Current LR = [0.0800002]\n",
      "Epoch 5: Train RMSLE = 0.6884706852808715, Test RMSLE = 0.6893075191993177 , Current LR = [0.1]\n",
      "Epoch 6: Train RMSLE = 0.6880786281454072, Test RMSLE = 0.6883744412637943 , Current LR = [0.0800002]\n",
      "Epoch 7: Train RMSLE = 0.6871726254996301, Test RMSLE = 0.6887541645783841 , Current LR = [0.06000040000000001]\n",
      "Epoch 8: Train RMSLE = 0.6861238504724139, Test RMSLE = 0.6883898411504795 , Current LR = [0.0400006]\n",
      "Epoch 9: Train RMSLE = 0.6847569100117662, Test RMSLE = 0.6892120573867908 , Current LR = [0.02000080000000002]\n",
      "Epoch 10: Train RMSLE = 0.6827360125809078, Test RMSLE = 0.6899582764395122 , Current LR = [1e-06]\n",
      "Minimum Train RMSLE so far: 0.6827360125809078\n",
      "Minimum Test RMSLE so far: 0.6879812699859111\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, test_loader, num_epochs=10)\n",
    "# Minimum Train RMSLE so far: 0.6845218561584729\n",
    "# Minimum Test RMSLE so far: 0.6898618382582543"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Assume `train` is your DataFrame loaded with the data\n",
    "X = train.drop(columns=['price'])\n",
    "y = np.log1p(train['price'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    # prevent negative predictions\n",
    "    preds = np.clip(preds, a_min=0, a_max=None)\n",
    "    return 'RMSLE', np.sqrt(np.mean(np.power((preds-labels), 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint, uniform\n",
    "\n",
    "param_dist = {\n",
    "    'max_depth': randint(0, 100),\n",
    "    'min_child_weight': randint(0, 100),\n",
    "    'subsample': uniform(0.6, 0.6),\n",
    "    'colsample_bytree': uniform(0.6, 0.6),\n",
    "    'learning_rate': uniform(0.01, 0.8),\n",
    "    'n_estimators': randint(0,1000)\n",
    "}\n",
    "\n",
    "# Create the RMSLE scorer\n",
    "def rmsle_scorer(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.power(y_pred - y_true, 2)))\n",
    "\n",
    "rmsle_scoring = make_scorer(rmsle_scorer, greater_is_better=False)\n",
    "\n",
    "# Initialize the XGBoost Regressor\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_jobs=10)\n",
    "\n",
    "# Setup RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_dist, \n",
    "                                   n_iter=100, scoring=rmsle_scoring, cv=3, verbose=3, random_state=42,n_jobs=10)\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_rmsle = rmsle_scorer(y_test, y_pred)\n",
    "\n",
    "print(\"Best model parameters:\", random_search.best_params_)\n",
    "print(\"Test RMSLE:\", test_rmsle)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
