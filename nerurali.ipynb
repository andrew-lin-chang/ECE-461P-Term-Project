{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "# from transformers import BertTokenizer, TFBertModel\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train=pd.read_csv('C:/Users/divya/OneDrive/Documents/UTAUSTIN/Semester 6/461p/ECE-461P-Term-Project/train_pp32.csv' ,delimiter=',')\n",
    "# test=pd.read_csv('C:/Users/divya/OneDrive/Documents/UTAUSTIN/Semester 6/461p/ECE-461P-Term-Project/test_pp32.csv' ,delimiter=',')\n",
    "\n",
    "train=pd.read_csv('C:/Users/divya/OneDrive/Documents/UTAUSTIN/Semester 6/461p/ECE-461P-Term-Project/train_pp2.csv' ,delimiter=',')\n",
    "test=pd.read_csv('C:/Users/divya/OneDrive/Documents/UTAUSTIN/Semester 6/461p/ECE-461P-Term-Project/test_pp2.csv' ,delimiter=',')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = train.drop(columns=['price'])\n",
    "y = ((train['price'])) # talk about this \n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train.astype(np.float32)).to(device)\n",
    "X_test_tensor = torch.tensor(X_test.astype(np.float32)).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values.astype(np.float32)).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.values.astype(np.float32)).to(device)\n",
    "\n",
    "# # Reshape y tensors to have the correct shape (n_samples, 1)\n",
    "y_train_tensor = y_train_tensor.view(-1, 1)\n",
    "y_test_tensor = y_test_tensor.view(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def rmsle_loss(y_pred, y_true):\n",
    "    # Add a small constant to avoid taking log of zero\n",
    "    epsilon = 1e-6\n",
    "    y_pred = torch.clamp(y_pred, min=epsilon, max=1e9)  # Clamp predictions to avoid log(0)\n",
    "    y_true = torch.clamp(y_true, min=epsilon, max=1e9)\n",
    "    return torch.sqrt(torch.mean((torch.log1p(y_pred) - torch.log1p(y_true)) ** 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "\n",
    "def train_model(model, train_loader, test_loader, num_epochs=5):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-6, momentum=0.8)\n",
    "    scheduler = CyclicLR(optimizer, base_lr=1e-6, max_lr=0.1, step_size_up=5*len(train_loader),\n",
    "                     mode='triangular', cycle_momentum=False)\n",
    "\n",
    "\n",
    "    min_train_rmsle = float('inf')\n",
    "    min_test_rmsle = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = rmsle_loss(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update learning rate at each batch\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_rmsle = total_train_loss / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                test_loss = rmsle_loss(outputs, targets)\n",
    "                total_test_loss += test_loss.item()\n",
    "\n",
    "        avg_test_rmsle = total_test_loss / len(test_loader)\n",
    "\n",
    "        if avg_train_rmsle < min_train_rmsle:\n",
    "            min_train_rmsle = avg_train_rmsle\n",
    "        if avg_test_rmsle < min_test_rmsle:\n",
    "            min_test_rmsle = avg_test_rmsle\n",
    "\n",
    "        print(f'Epoch {epoch+1}: Train RMSLE = {avg_train_rmsle}, Test RMSLE = {avg_test_rmsle} , Current LR = {scheduler.get_last_lr()}')\n",
    "\n",
    "    print(f'Minimum Train RMSLE so far: {min_train_rmsle}')\n",
    "    print(f'Minimum Test RMSLE so far: {min_test_rmsle}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(model, train_loader, test_loader, num_epochs=50):\n",
    "#     model.to(device)\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         total_train_loss = 0\n",
    "#         for inputs, targets in train_loader:\n",
    "#             inputs, targets = inputs.to(device), targets.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = rmsle_loss(outputs, targets)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             total_train_loss += loss.item()\n",
    "\n",
    "#         avg_train_rmsle = total_train_loss / len(train_loader)\n",
    "#         total_test_loss = 0\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             for inputs, targets in test_loader:\n",
    "#                 inputs, targets = inputs.to(device), targets.to(device)\n",
    "#                 outputs = model(inputs)\n",
    "#                 test_loss = rmsle_loss(outputs, targets)\n",
    "#                 total_test_loss += test_loss.item()\n",
    "\n",
    "#         avg_test_rmsle = total_test_loss / len(test_loader)\n",
    "#         scheduler.step(avg_test_rmsle)\n",
    "\n",
    "#         print(f'Epoch {epoch+1}: Train RMSLE = {avg_train_rmsle}, Test RMSLE = {avg_test_rmsle}')\n",
    "\n",
    "#     print(f'Best Test RMSLE so far: {scheduler.best}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X_train.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PricePredictionModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size=1):\n",
    "        super(PricePredictionModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.layer2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.layer3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.layer4 = nn.Linear(64, 32)\n",
    "        self.bn4 = nn.BatchNorm1d(32)\n",
    "        self.dropout4 = nn.Dropout(0.1)\n",
    "        \n",
    "        self.output_layer = nn.Linear(32, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.layer1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.bn2(self.layer2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.bn3(self.layer3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.bn4(self.layer4(x)))\n",
    "        x = self.dropout4(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train RMSLE = 0.7103170956413825, Test RMSLE = 0.6860611839599665\n",
      "Epoch 2: Train RMSLE = 0.6905549636505591, Test RMSLE = 0.6841801157286848\n",
      "Epoch 3: Train RMSLE = 0.6894629118511327, Test RMSLE = 0.683762296058531\n",
      "Epoch 4: Train RMSLE = 0.6887406774495348, Test RMSLE = 0.6837902820019699\n",
      "Epoch 5: Train RMSLE = 0.6881720342230563, Test RMSLE = 0.6835961619336686\n",
      "Epoch 6: Train RMSLE = 0.6878651257972417, Test RMSLE = 0.6838404884550311\n",
      "Epoch 7: Train RMSLE = 0.6876620969280219, Test RMSLE = 0.6838252511346595\n",
      "Epoch 8: Train RMSLE = 0.6873545684333386, Test RMSLE = 0.6835608863926338\n",
      "Epoch 9: Train RMSLE = 0.6868888843362309, Test RMSLE = 0.6832752976677257\n",
      "Epoch 10: Train RMSLE = 0.6866839122186246, Test RMSLE = 0.6832874632584035\n",
      "Best Test RMSLE so far: 0.6832752976677257\n"
     ]
    }
   ],
   "source": [
    "model = PricePredictionModel(num_features).to(device)\n",
    "\n",
    "# Use the training function already provided\n",
    "train_model(model, train_loader, test_loader, num_epochs=10)\n",
    "\n",
    "#Epoch 5: Train RMSLE = 0.6883012584179728, Test RMSLE = 0.6835928453002499\n",
    "# Best Test RMSLE so far: 0.6832752976677257\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedPricePredictionModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size=1):\n",
    "        super(EnhancedPricePredictionModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        \n",
    "        self.layer3 = nn.Linear(256, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.dropout3 = nn.Dropout(0.1)\n",
    "        \n",
    "        self.layer4 = nn.Linear(128, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.dropout4 = nn.Dropout(0.1)\n",
    "        \n",
    "        self.output_layer = nn.Linear(64, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.layer1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.bn2(self.layer2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.bn3(self.layer3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.bn4(self.layer4(x)))\n",
    "        x = self.dropout4(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EnhancedPricePredictionModel(num_features).to(device)\n",
    "\n",
    "# Use the training function already provided\n",
    "train_model(model, train_loader, test_loader, num_epochs=10)\n",
    "# Epoch 1: Train RMSLE = 0.701097307975991, Test RMSLE = 0.6841780046707775\n",
    "# Epoch 2: Train RMSLE = 0.6888048825449483, Test RMSLE = 0.6840163404823544\n",
    "# Epoch 3: Train RMSLE = 0.6878524877785998, Test RMSLE = 0.683792892498503\n",
    "# Epoch 4: Train RMSLE = 0.6872429579799324, Test RMSLE = 0.683677059799721\n",
    "# Epoch 5: Train RMSLE = 0.6869078011756413, Test RMSLE = 0.6837233345654394\n",
    "# Epoch 6: Train RMSLE = 0.6863807070894459, Test RMSLE = 0.6837847139004105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.astype(np.float32)).unsqueeze(1)  # Add sequence dimension\n",
    "X_test_tensor = torch.tensor(X_test.astype(np.float32)).unsqueeze(1)  # Add sequence dimension\n",
    "y_train_tensor = torch.tensor(y_train.values.astype(np.float32)).view(-1, 1)\n",
    "y_test_tensor = torch.tensor(y_test.values.astype(np.float32)).view(-1, 1)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length, input_dim)\n",
    "        out, _ = self.rnn(x)  # out shape: (batch_size, sequence_length, hidden_dim)\n",
    "        out = out[:, -1, :]  # get the last sequence output\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length, input_dim)\n",
    "        out, (hn, cn) = self.lstm(x)  # out shape: (batch_size, sequence_length, hidden_dim)\n",
    "        out = out[:, -1, :]  # get the last sequence output\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train RMSLE = 0.9801828821023003, Test RMSLE = 0.7010385301054224 , Current LR = [0.02000080000000002]\n",
      "Epoch 2: Train RMSLE = 0.6950768298166766, Test RMSLE = 0.6910986202384739 , Current LR = [0.0400006]\n",
      "Epoch 3: Train RMSLE = 0.6902512350930401, Test RMSLE = 0.6890711715005972 , Current LR = [0.06000040000000001]\n",
      "Epoch 4: Train RMSLE = 0.6891211542877781, Test RMSLE = 0.6887067230965405 , Current LR = [0.0800002]\n",
      "Epoch 5: Train RMSLE = 0.6886812580246937, Test RMSLE = 0.6881854928252444 , Current LR = [0.1]\n",
      "Epoch 6: Train RMSLE = 0.6884077206164756, Test RMSLE = 0.6878372403099855 , Current LR = [0.0800002]\n",
      "Epoch 7: Train RMSLE = 0.6881301104061287, Test RMSLE = 0.6875795655843301 , Current LR = [0.06000040000000001]\n",
      "Epoch 8: Train RMSLE = 0.6876860214244988, Test RMSLE = 0.6875404149767607 , Current LR = [0.0400006]\n",
      "Epoch 9: Train RMSLE = 0.6875653081028397, Test RMSLE = 0.6874087602929898 , Current LR = [0.02000080000000002]\n",
      "Epoch 10: Train RMSLE = 0.6873640940228356, Test RMSLE = 0.6873739421027812 , Current LR = [1e-06]\n",
      "Minimum Train RMSLE so far: 0.6873640940228356\n",
      "Minimum Test RMSLE so far: 0.6873739421027812\n"
     ]
    }
   ],
   "source": [
    "model = LSTMModel(input_dim=num_features, hidden_dim=10, output_dim=1).to(device)  # Adjust dimensions as necessary\n",
    "train_model(model, train_loader, test_loader, num_epochs=10)\n",
    "# Best Test RMSLE so far: 0.6883052102114385\n",
    "\n",
    "# Minimum Train RMSLE so far: 0.6873640940228356\n",
    "# Minimum Test RMSLE so far: 0.6873739421027812\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train RMSLE = 0.89248368301251, Test RMSLE = 0.6959494175987978 , Current LR = [0.02000080000000002]\n",
      "Epoch 2: Train RMSLE = 0.6926944601239902, Test RMSLE = 0.6905906675866166 , Current LR = [0.0400006]\n",
      "Epoch 3: Train RMSLE = 0.690474758986654, Test RMSLE = 0.6897507240505324 , Current LR = [0.06000040000000001]\n",
      "Epoch 4: Train RMSLE = 0.6898817215179194, Test RMSLE = 0.6892154644029093 , Current LR = [0.0800002]\n",
      "Epoch 5: Train RMSLE = 0.6897256960639672, Test RMSLE = 0.6890912703248082 , Current LR = [0.1]\n",
      "Epoch 6: Train RMSLE = 0.6893705928934606, Test RMSLE = 0.6886392525592259 , Current LR = [0.0800002]\n",
      "Epoch 7: Train RMSLE = 0.6889364930379092, Test RMSLE = 0.6883083894828228 , Current LR = [0.06000040000000001]\n",
      "Epoch 8: Train RMSLE = 0.6886863322390908, Test RMSLE = 0.6880548574035549 , Current LR = [0.0400006]\n",
      "Epoch 9: Train RMSLE = 0.6883312759964292, Test RMSLE = 0.6878094568014474 , Current LR = [0.02000080000000002]\n",
      "Epoch 10: Train RMSLE = 0.6880693981135202, Test RMSLE = 0.6876124085880347 , Current LR = [1e-06]\n",
      "Minimum Train RMSLE so far: 0.6880693981135202\n",
      "Minimum Test RMSLE so far: 0.6876124085880347\n"
     ]
    }
   ],
   "source": [
    "model = RNNModel(input_dim=num_features, hidden_dim=10, output_dim=1).to(device)  # Adjust dimensions as necessary\n",
    "train_model(model, train_loader, test_loader, num_epochs=10)\n",
    "#Best Test RMSLE so far: 0.9206008657493835\n",
    "\n",
    "# Minimum Train RMSLE so far: 0.6880693981135202\n",
    "# Minimum Test RMSLE so far: 0.6876124085880347"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RBNFModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1):\n",
    "        super(RBNFModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length, input_dim)\n",
    "        out, (hn, cn) = self.lstm(x)  # out shape: (batch_size, sequence_length, hidden_dim)\n",
    "        out = out[:, -1, :]  # we only use the output of the last time step\n",
    "        out = self.batch_norm(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train RMSLE = 0.8608611680359589, Test RMSLE = 0.6921516187288222 , Current LR = [0.02000080000000002]\n",
      "Epoch 2: Train RMSLE = 0.691954627581605, Test RMSLE = 0.6891397957708725 , Current LR = [0.0400006]\n",
      "Epoch 3: Train RMSLE = 0.6908542909358015, Test RMSLE = 0.6892781519918666 , Current LR = [0.06000040000000001]\n",
      "Epoch 4: Train RMSLE = 0.6904197056202591, Test RMSLE = 0.688713728794577 , Current LR = [0.0800002]\n",
      "Epoch 5: Train RMSLE = 0.6902876732756371, Test RMSLE = 0.6883867871847825 , Current LR = [0.1]\n",
      "Epoch 6: Train RMSLE = 0.6900170087999694, Test RMSLE = 0.6879308694598999 , Current LR = [0.0800002]\n",
      "Epoch 7: Train RMSLE = 0.6896799886313844, Test RMSLE = 0.6888296482588969 , Current LR = [0.06000040000000001]\n",
      "Epoch 8: Train RMSLE = 0.6892936175813106, Test RMSLE = 0.6877083220540365 , Current LR = [0.0400006]\n",
      "Epoch 9: Train RMSLE = 0.6889687462666292, Test RMSLE = 0.6873600878434175 , Current LR = [0.02000080000000002]\n",
      "Epoch 10: Train RMSLE = 0.6887654505139725, Test RMSLE = 0.6873131738441437 , Current LR = [1e-06]\n",
      "Minimum Train RMSLE so far: 0.6887654505139725\n",
      "Minimum Test RMSLE so far: 0.6873131738441437\n"
     ]
    }
   ],
   "source": [
    "model = RBNFModel(input_dim=num_features, hidden_dim=10, output_dim=1).to(device)  # Adjust dimensions as necessary\n",
    "train_model(model, train_loader, test_loader, num_epochs=10)\n",
    "\n",
    "# Minimum Train RMSLE so far: 0.6887654505139725\n",
    "# Minimum Test RMSLE so far: 0.6873131738441437"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-tabnet\n",
      "  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from pytorch-tabnet) (1.26.4)\n",
      "Requirement already satisfied: scikit_learn>0.21 in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from pytorch-tabnet) (1.4.2)\n",
      "Requirement already satisfied: scipy>1.4 in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from pytorch-tabnet) (1.13.0)\n",
      "Requirement already satisfied: torch>=1.3 in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from pytorch-tabnet) (2.2.2+cu118)\n",
      "Requirement already satisfied: tqdm>=4.36 in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from pytorch-tabnet) (4.66.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from scikit_learn>0.21->pytorch-tabnet) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from scikit_learn>0.21->pytorch-tabnet) (3.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (2024.3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from tqdm>=4.36->pytorch-tabnet) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from jinja2->torch>=1.3->pytorch-tabnet) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from sympy->torch>=1.3->pytorch-tabnet) (1.3.0)\n",
      "Downloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n",
      "   ---------------------------------------- 0.0/44.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 44.5/44.5 kB 2.1 MB/s eta 0:00:00\n",
      "Installing collected packages: pytorch-tabnet\n",
      "Successfully installed pytorch-tabnet-4.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-tabnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\divya\\OneDrive\\Documents\\UTAUSTIN\\Semester 6\\461p\\ECE-461P-Term-Project\\.venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from pytorch_tabnet.callbacks import Callback\n",
    "\n",
    "X = train.drop(columns=['price'])\n",
    "y = np.log1p((train['price'])).values.reshape(-1, 1) # talk about this \n",
    "\n",
    "# Initialize TabNetRegressor\n",
    "model = TabNetRegressor(device_name='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.52663 | eval_rmse: 0.6961  |  0:01:28s\n",
      "epoch 1  | loss: 0.48746 | eval_rmse: 0.69314 |  0:02:48s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m  \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m  \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m  \u001b[49m\u001b[43meval_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m  \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrmse\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m  \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Early stopping based on validation loss\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m  \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust according to your GPU memory\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m  \u001b[49m\u001b[43mvirtual_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Mini batch size for \"Ghost Batch Normalization\"\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m  \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdrop_last\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Predict and evaluate\u001b[39;00m\n\u001b[0;32m     14\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\divya\\OneDrive\\Documents\\UTAUSTIN\\Semester 6\\461p\\ECE-461P-Term-Project\\.venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:258\u001b[0m, in \u001b[0;36mTabModel.fit\u001b[1;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations, compute_importance)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_epochs):\n\u001b[0;32m    254\u001b[0m \n\u001b[0;32m    255\u001b[0m     \u001b[38;5;66;03m# Call method on_epoch_begin for all callbacks\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_container\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch_idx)\n\u001b[1;32m--> 258\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# Apply predict epoch to all eval sets\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m eval_name, valid_dataloader \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(eval_names, valid_dataloaders):\n",
      "File \u001b[1;32mc:\\Users\\divya\\OneDrive\\Documents\\UTAUSTIN\\Semester 6\\461p\\ECE-461P-Term-Project\\.venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:489\u001b[0m, in \u001b[0;36mTabModel._train_epoch\u001b[1;34m(self, train_loader)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m    487\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_container\u001b[38;5;241m.\u001b[39mon_batch_begin(batch_idx)\n\u001b[1;32m--> 489\u001b[0m     batch_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_container\u001b[38;5;241m.\u001b[39mon_batch_end(batch_idx, batch_logs)\n\u001b[0;32m    493\u001b[0m epoch_logs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n",
      "File \u001b[1;32mc:\\Users\\divya\\OneDrive\\Documents\\UTAUSTIN\\Semester 6\\461p\\ECE-461P-Term-Project\\.venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:534\u001b[0m, in \u001b[0;36mTabModel._train_batch\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    531\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda_sparse \u001b[38;5;241m*\u001b[39m M_loss\n\u001b[0;32m    533\u001b[0m \u001b[38;5;66;03m# Perform backward pass and optimization\u001b[39;00m\n\u001b[1;32m--> 534\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_value:\n\u001b[0;32m    536\u001b[0m     clip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_value)\n",
      "File \u001b[1;32mc:\\Users\\divya\\OneDrive\\Documents\\UTAUSTIN\\Semester 6\\461p\\ECE-461P-Term-Project\\.venv\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\divya\\OneDrive\\Documents\\UTAUSTIN\\Semester 6\\461p\\ECE-461P-Term-Project\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit(\n",
    "  X_train, y_train,\n",
    "  eval_set=[(X_test, y_test)],\n",
    "  eval_name=['eval'],\n",
    "  eval_metric=['rmse'],\n",
    "  max_epochs=100,\n",
    "  patience=50,  # Early stopping based on validation loss\n",
    "  batch_size=512,  # Adjust according to your GPU memory\n",
    "  virtual_batch_size=128,  # Mini batch size for \"Ghost Batch Normalization\"\n",
    "  num_workers=0,\n",
    "  drop_last=False\n",
    ")\n",
    "# Predict and evaluate\n",
    "predictions = model.predict(X_test)\n",
    "rmse = sqrt(mean_squared_error(y_test, predictions))\n",
    "print(f\"Test RMSE: {rmse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
