{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "# from transformers import BertTokenizer, TFBertModel\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train=pd.read_csv('C:/Users/divya/OneDrive/Documents/UTAUSTIN/Semester 6/461p/ECE-461P-Term-Project/train_pp32.csv' ,delimiter=',')\n",
    "# test=pd.read_csv('C:/Users/divya/OneDrive/Documents/UTAUSTIN/Semester 6/461p/ECE-461P-Term-Project/test_pp32.csv' ,delimiter=',')\n",
    "\n",
    "train=pd.read_csv('C:/Users/divya/OneDrive/Documents/UTAUSTIN/Semester 6/461p/ECE-461P-Term-Project/train_pp2.csv' ,delimiter=',')\n",
    "test=pd.read_csv('C:/Users/divya/OneDrive/Documents/UTAUSTIN/Semester 6/461p/ECE-461P-Term-Project/test_pp2.csv' ,delimiter=',')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = train.drop(columns=['price'])\n",
    "y = ((train['price'])) # talk about this \n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train.astype(np.float32)).to(device)\n",
    "X_test_tensor = torch.tensor(X_test.astype(np.float32)).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values.astype(np.float32)).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.values.astype(np.float32)).to(device)\n",
    "\n",
    "# # Reshape y tensors to have the correct shape (n_samples, 1)\n",
    "y_train_tensor = y_train_tensor.view(-1, 1)\n",
    "y_test_tensor = y_test_tensor.view(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def rmsle_loss(y_pred, y_true):\n",
    "    # Add a small constant to avoid taking log of zero\n",
    "    epsilon = 1e-6\n",
    "    y_pred = torch.clamp(y_pred, min=epsilon, max=1e9)  # Clamp predictions to avoid log(0)\n",
    "    y_true = torch.clamp(y_true, min=epsilon, max=1e9)\n",
    "    return torch.sqrt(torch.mean((torch.log1p(y_pred) - torch.log1p(y_true)) ** 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "\n",
    "def train_model(model, train_loader, test_loader, num_epochs=5):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-6, momentum=0.8)\n",
    "    scheduler = CyclicLR(optimizer, base_lr=1e-6, max_lr=0.1, step_size_up=5*len(train_loader),\n",
    "                     mode='triangular', cycle_momentum=False)\n",
    "\n",
    "\n",
    "    min_train_rmsle = float('inf')\n",
    "    min_test_rmsle = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = rmsle_loss(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update learning rate at each batch\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_rmsle = total_train_loss / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                test_loss = rmsle_loss(outputs, targets)\n",
    "                total_test_loss += test_loss.item()\n",
    "\n",
    "        avg_test_rmsle = total_test_loss / len(test_loader)\n",
    "\n",
    "        if avg_train_rmsle < min_train_rmsle:\n",
    "            min_train_rmsle = avg_train_rmsle\n",
    "        if avg_test_rmsle < min_test_rmsle:\n",
    "            min_test_rmsle = avg_test_rmsle\n",
    "\n",
    "        print(f'Epoch {epoch+1}: Train RMSLE = {avg_train_rmsle}, Test RMSLE = {avg_test_rmsle} , Current LR = {scheduler.get_last_lr()}')\n",
    "\n",
    "    print(f'Minimum Train RMSLE so far: {min_train_rmsle}')\n",
    "    print(f'Minimum Test RMSLE so far: {min_test_rmsle}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(model, train_loader, test_loader, num_epochs=50):\n",
    "#     model.to(device)\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         total_train_loss = 0\n",
    "#         for inputs, targets in train_loader:\n",
    "#             inputs, targets = inputs.to(device), targets.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = rmsle_loss(outputs, targets)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             total_train_loss += loss.item()\n",
    "\n",
    "#         avg_train_rmsle = total_train_loss / len(train_loader)\n",
    "#         total_test_loss = 0\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             for inputs, targets in test_loader:\n",
    "#                 inputs, targets = inputs.to(device), targets.to(device)\n",
    "#                 outputs = model(inputs)\n",
    "#                 test_loss = rmsle_loss(outputs, targets)\n",
    "#                 total_test_loss += test_loss.item()\n",
    "\n",
    "#         avg_test_rmsle = total_test_loss / len(test_loader)\n",
    "#         scheduler.step(avg_test_rmsle)\n",
    "\n",
    "#         print(f'Epoch {epoch+1}: Train RMSLE = {avg_train_rmsle}, Test RMSLE = {avg_test_rmsle}')\n",
    "\n",
    "#     print(f'Best Test RMSLE so far: {scheduler.best}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X_train.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PricePredictionModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size=1):\n",
    "        super(PricePredictionModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.layer2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.layer3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.layer4 = nn.Linear(64, 32)\n",
    "        self.bn4 = nn.BatchNorm1d(32)\n",
    "        self.dropout4 = nn.Dropout(0.1)\n",
    "        \n",
    "        self.output_layer = nn.Linear(32, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.layer1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.bn2(self.layer2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.bn3(self.layer3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.bn4(self.layer4(x)))\n",
    "        x = self.dropout4(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train RMSLE = 0.7103170956413825, Test RMSLE = 0.6860611839599665\n",
      "Epoch 2: Train RMSLE = 0.6905549636505591, Test RMSLE = 0.6841801157286848\n",
      "Epoch 3: Train RMSLE = 0.6894629118511327, Test RMSLE = 0.683762296058531\n",
      "Epoch 4: Train RMSLE = 0.6887406774495348, Test RMSLE = 0.6837902820019699\n",
      "Epoch 5: Train RMSLE = 0.6881720342230563, Test RMSLE = 0.6835961619336686\n",
      "Epoch 6: Train RMSLE = 0.6878651257972417, Test RMSLE = 0.6838404884550311\n",
      "Epoch 7: Train RMSLE = 0.6876620969280219, Test RMSLE = 0.6838252511346595\n",
      "Epoch 8: Train RMSLE = 0.6873545684333386, Test RMSLE = 0.6835608863926338\n",
      "Epoch 9: Train RMSLE = 0.6868888843362309, Test RMSLE = 0.6832752976677257\n",
      "Epoch 10: Train RMSLE = 0.6866839122186246, Test RMSLE = 0.6832874632584035\n",
      "Best Test RMSLE so far: 0.6832752976677257\n"
     ]
    }
   ],
   "source": [
    "model = PricePredictionModel(num_features).to(device)\n",
    "\n",
    "# Use the training function already provided\n",
    "train_model(model, train_loader, test_loader, num_epochs=10)\n",
    "\n",
    "#Epoch 5: Train RMSLE = 0.6883012584179728, Test RMSLE = 0.6835928453002499\n",
    "# Best Test RMSLE so far: 0.6832752976677257\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedPricePredictionModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size=1):\n",
    "        super(EnhancedPricePredictionModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        \n",
    "        self.layer3 = nn.Linear(256, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.dropout3 = nn.Dropout(0.1)\n",
    "        \n",
    "        self.layer4 = nn.Linear(128, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.dropout4 = nn.Dropout(0.1)\n",
    "        \n",
    "        self.output_layer = nn.Linear(64, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.layer1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.bn2(self.layer2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.bn3(self.layer3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.bn4(self.layer4(x)))\n",
    "        x = self.dropout4(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train RMSLE = 0.7539822288183858, Test RMSLE = 0.6851938397268962 , Current LR = [0.02000080000000002]\n",
      "Epoch 2: Train RMSLE = 0.6899419926804423, Test RMSLE = 0.6844406957920247 , Current LR = [0.0400006]\n",
      "Epoch 3: Train RMSLE = 0.6882493528543755, Test RMSLE = 0.6845732714257409 , Current LR = [0.06000040000000001]\n",
      "Epoch 4: Train RMSLE = 0.6874938286536473, Test RMSLE = 0.6836005338772091 , Current LR = [0.0800002]\n",
      "Epoch 5: Train RMSLE = 0.6868805261900853, Test RMSLE = 0.6843336063624458 , Current LR = [0.1]\n",
      "Epoch 6: Train RMSLE = 0.686066268062078, Test RMSLE = 0.683395181901614 , Current LR = [0.0800002]\n",
      "Epoch 7: Train RMSLE = 0.685587858133876, Test RMSLE = 0.6836968424531124 , Current LR = [0.06000040000000001]\n",
      "Epoch 8: Train RMSLE = 0.6851818325337489, Test RMSLE = 0.6835474956886226 , Current LR = [0.0400006]\n",
      "Epoch 9: Train RMSLE = 0.6848142597793547, Test RMSLE = 0.683199419487466 , Current LR = [0.02000080000000002]\n",
      "Epoch 10: Train RMSLE = 0.6845869874318821, Test RMSLE = 0.6833885734245624 , Current LR = [1e-06]\n",
      "Epoch 11: Train RMSLE = 0.6845028053120717, Test RMSLE = 0.6833720337663878 , Current LR = [0.02000080000000002]\n",
      "Epoch 12: Train RMSLE = 0.6846912156911822, Test RMSLE = 0.6834351204969015 , Current LR = [0.04000060000000004]\n",
      "Epoch 13: Train RMSLE = 0.6848654789352313, Test RMSLE = 0.68343514243415 , Current LR = [0.06000039999999997]\n",
      "Epoch 14: Train RMSLE = 0.6850439287451996, Test RMSLE = 0.68492442354529 , Current LR = [0.0800002]\n",
      "Epoch 15: Train RMSLE = 0.6851324817364135, Test RMSLE = 0.6835727048928398 , Current LR = [0.1]\n",
      "Epoch 16: Train RMSLE = 0.6850305952022973, Test RMSLE = 0.6838128294386407 , Current LR = [0.0800002]\n",
      "Epoch 17: Train RMSLE = 0.6848094569454851, Test RMSLE = 0.683875931276275 , Current LR = [0.06000039999999997]\n",
      "Epoch 18: Train RMSLE = 0.6845211650616568, Test RMSLE = 0.6836076823488629 , Current LR = [0.04000060000000004]\n",
      "Epoch 19: Train RMSLE = 0.6841905335246578, Test RMSLE = 0.6834219732343039 , Current LR = [0.02000080000000002]\n",
      "Epoch 20: Train RMSLE = 0.6837380584807271, Test RMSLE = 0.6832889288293137 , Current LR = [1e-06]\n",
      "Epoch 21: Train RMSLE = 0.6836483555577169, Test RMSLE = 0.683457553433029 , Current LR = [0.02000080000000002]\n",
      "Epoch 22: Train RMSLE = 0.683869449213599, Test RMSLE = 0.6836809313134564 , Current LR = [0.04000060000000004]\n",
      "Epoch 23: Train RMSLE = 0.6841514181348598, Test RMSLE = 0.6839851870587058 , Current LR = [0.06000039999999997]\n",
      "Epoch 24: Train RMSLE = 0.6845115706499219, Test RMSLE = 0.6837305418977098 , Current LR = [0.0800002]\n",
      "Epoch 25: Train RMSLE = 0.6846528727459648, Test RMSLE = 0.6837965635571219 , Current LR = [0.1]\n",
      "Epoch 26: Train RMSLE = 0.6844546136832187, Test RMSLE = 0.6847410276712388 , Current LR = [0.0800002]\n",
      "Epoch 27: Train RMSLE = 0.6843101272767828, Test RMSLE = 0.6839562919461982 , Current LR = [0.06000039999999997]\n",
      "Epoch 28: Train RMSLE = 0.6839007738470588, Test RMSLE = 0.6835686148870955 , Current LR = [0.04000060000000004]\n",
      "Epoch 29: Train RMSLE = 0.6834825290630417, Test RMSLE = 0.6838862939139811 , Current LR = [0.02000080000000002]\n",
      "Epoch 30: Train RMSLE = 0.683162161842528, Test RMSLE = 0.6837213695327331 , Current LR = [1e-06]\n",
      "Epoch 31: Train RMSLE = 0.6830647256232105, Test RMSLE = 0.6840891762306424 , Current LR = [0.02000079999999993]\n",
      "Epoch 32: Train RMSLE = 0.683372942018005, Test RMSLE = 0.6837840807040395 , Current LR = [0.04000060000000004]\n",
      "Epoch 33: Train RMSLE = 0.6835249579378108, Test RMSLE = 0.6841215774617734 , Current LR = [0.06000039999999997]\n",
      "Epoch 34: Train RMSLE = 0.683933489871457, Test RMSLE = 0.6844228592153533 , Current LR = [0.08000020000000008]\n",
      "Epoch 35: Train RMSLE = 0.6841154166393476, Test RMSLE = 0.684757529504973 , Current LR = [0.1]\n",
      "Epoch 36: Train RMSLE = 0.6840966893008966, Test RMSLE = 0.6842065378920572 , Current LR = [0.08000020000000008]\n",
      "Epoch 37: Train RMSLE = 0.6837174732656529, Test RMSLE = 0.6839335183687019 , Current LR = [0.06000039999999997]\n",
      "Epoch 38: Train RMSLE = 0.6833600085241018, Test RMSLE = 0.6838827320781962 , Current LR = [0.04000060000000004]\n",
      "Epoch 39: Train RMSLE = 0.6829450783724541, Test RMSLE = 0.6838193757828961 , Current LR = [0.02000079999999993]\n",
      "Epoch 40: Train RMSLE = 0.6824771196459373, Test RMSLE = 0.6842055599222574 , Current LR = [1e-06]\n",
      "Epoch 41: Train RMSLE = 0.6824070625851396, Test RMSLE = 0.6842310016443174 , Current LR = [0.02000079999999993]\n",
      "Epoch 42: Train RMSLE = 0.682591468557485, Test RMSLE = 0.6842721787557449 , Current LR = [0.04000060000000004]\n",
      "Epoch 43: Train RMSLE = 0.683007461680658, Test RMSLE = 0.6844511356336047 , Current LR = [0.06000039999999997]\n",
      "Epoch 44: Train RMSLE = 0.6833417027279883, Test RMSLE = 0.6839068744926305 , Current LR = [0.08000020000000008]\n",
      "Epoch 45: Train RMSLE = 0.6836555799609397, Test RMSLE = 0.684807276117674 , Current LR = [0.1]\n",
      "Epoch 46: Train RMSLE = 0.6835845809529756, Test RMSLE = 0.6842666148094161 , Current LR = [0.08000020000000008]\n",
      "Epoch 47: Train RMSLE = 0.683194630330091, Test RMSLE = 0.6839981077273674 , Current LR = [0.06000039999999997]\n",
      "Epoch 48: Train RMSLE = 0.6826265334918236, Test RMSLE = 0.6847261244839195 , Current LR = [0.04000060000000004]\n",
      "Epoch 49: Train RMSLE = 0.6823050463254883, Test RMSLE = 0.685146133208798 , Current LR = [0.02000079999999993]\n",
      "Epoch 50: Train RMSLE = 0.6818514334236195, Test RMSLE = 0.6846960176737579 , Current LR = [1e-06]\n",
      "Minimum Train RMSLE so far: 0.6818514334236195\n",
      "Minimum Test RMSLE so far: 0.683199419487466\n"
     ]
    }
   ],
   "source": [
    "model = EnhancedPricePredictionModel(num_features).to(device)\n",
    "\n",
    "# Use the training function already provided\n",
    "train_model(model, train_loader, test_loader, num_epochs=50)\n",
    "# Epoch 1: Train RMSLE = 0.701097307975991, Test RMSLE = 0.6841780046707775\n",
    "# Epoch 2: Train RMSLE = 0.6888048825449483, Test RMSLE = 0.6840163404823544\n",
    "# Epoch 3: Train RMSLE = 0.6878524877785998, Test RMSLE = 0.683792892498503\n",
    "# Epoch 4: Train RMSLE = 0.6872429579799324, Test RMSLE = 0.683677059799721\n",
    "# Epoch 5: Train RMSLE = 0.6869078011756413, Test RMSLE = 0.6837233345654394\n",
    "# Epoch 6: Train RMSLE = 0.6863807070894459, Test RMSLE = 0.6837847139004105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.astype(np.float32)).unsqueeze(1)  # Add sequence dimension\n",
    "X_test_tensor = torch.tensor(X_test.astype(np.float32)).unsqueeze(1)  # Add sequence dimension\n",
    "y_train_tensor = torch.tensor(y_train.values.astype(np.float32)).view(-1, 1)\n",
    "y_test_tensor = torch.tensor(y_test.values.astype(np.float32)).view(-1, 1)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length, input_dim)\n",
    "        out, _ = self.rnn(x)  # out shape: (batch_size, sequence_length, hidden_dim)\n",
    "        out = out[:, -1, :]  # get the last sequence output\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length, input_dim)\n",
    "        out, (hn, cn) = self.lstm(x)  # out shape: (batch_size, sequence_length, hidden_dim)\n",
    "        out = out[:, -1, :]  # get the last sequence output\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train RMSLE = 0.9505505237734364, Test RMSLE = 0.697044079488693 , Current LR = [0.02000080000000002]\n",
      "Epoch 2: Train RMSLE = 0.6932499246288626, Test RMSLE = 0.6904735023100457 , Current LR = [0.0400006]\n",
      "Epoch 3: Train RMSLE = 0.6900384213120617, Test RMSLE = 0.6890186226586268 , Current LR = [0.06000040000000001]\n",
      "Epoch 4: Train RMSLE = 0.6887344956531883, Test RMSLE = 0.6880314034546913 , Current LR = [0.0800002]\n",
      "Epoch 5: Train RMSLE = 0.6882432286220607, Test RMSLE = 0.6881312627368494 , Current LR = [0.1]\n",
      "Epoch 6: Train RMSLE = 0.6879552400327036, Test RMSLE = 0.6877571556359303 , Current LR = [0.0800002]\n",
      "Epoch 7: Train RMSLE = 0.6875714813525351, Test RMSLE = 0.6875845276555607 , Current LR = [0.06000040000000001]\n",
      "Epoch 8: Train RMSLE = 0.6872967017775471, Test RMSLE = 0.6873673786407268 , Current LR = [0.0400006]\n",
      "Epoch 9: Train RMSLE = 0.6869023333384512, Test RMSLE = 0.6874346182975756 , Current LR = [0.02000080000000002]\n",
      "Epoch 10: Train RMSLE = 0.6867546385123663, Test RMSLE = 0.6873341582473509 , Current LR = [1e-06]\n",
      "Minimum Train RMSLE so far: 0.6867546385123663\n",
      "Minimum Test RMSLE so far: 0.6873341582473509\n"
     ]
    }
   ],
   "source": [
    "model = LSTMModel(input_dim=num_features, hidden_dim=100, output_dim=1).to(device)  # Adjust dimensions as necessary\n",
    "train_model(model, train_loader, test_loader, num_epochs=10)\n",
    "# Best Test RMSLE so far: 0.6883052102114385\n",
    "\n",
    "# Minimum Train RMSLE so far: 0.6873640940228356\n",
    "# Minimum Test RMSLE so far: 0.6873739421027812\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train RMSLE = 0.8828789826205348, Test RMSLE = 0.6956600377435262 , Current LR = [0.02000080000000002]\n",
      "Epoch 2: Train RMSLE = 0.6926597734233993, Test RMSLE = 0.6904328210895937 , Current LR = [0.0400006]\n",
      "Epoch 3: Train RMSLE = 0.690628907961852, Test RMSLE = 0.690253002943668 , Current LR = [0.06000040000000001]\n",
      "Epoch 4: Train RMSLE = 0.6901779458925653, Test RMSLE = 0.689415318493647 , Current LR = [0.0800002]\n",
      "Epoch 5: Train RMSLE = 0.6900961761549715, Test RMSLE = 0.6895427988699245 , Current LR = [0.1]\n",
      "Epoch 6: Train RMSLE = 0.6896527157287613, Test RMSLE = 0.6887947437768569 , Current LR = [0.0800002]\n",
      "Epoch 7: Train RMSLE = 0.6892808137731435, Test RMSLE = 0.6886071407885574 , Current LR = [0.06000040000000001]\n",
      "Epoch 8: Train RMSLE = 0.6889113583781153, Test RMSLE = 0.6881322431807363 , Current LR = [0.0400006]\n",
      "Epoch 9: Train RMSLE = 0.688569586635652, Test RMSLE = 0.6878116669206679 , Current LR = [0.02000080000000002]\n",
      "Epoch 10: Train RMSLE = 0.6881853451214747, Test RMSLE = 0.6876033042565904 , Current LR = [1e-06]\n",
      "Minimum Train RMSLE so far: 0.6881853451214747\n",
      "Minimum Test RMSLE so far: 0.6876033042565904\n"
     ]
    }
   ],
   "source": [
    "model = RNNModel(input_dim=num_features, hidden_dim=10, output_dim=1).to(device)  # Adjust dimensions as necessary\n",
    "train_model(model, train_loader, test_loader, num_epochs=10)\n",
    "#Best Test RMSLE so far: 0.9206008657493835\n",
    "\n",
    "# Minimum Train RMSLE so far: 0.6880693981135202\n",
    "# Minimum Test RMSLE so far: 0.6876124085880347"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RBNFModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1):\n",
    "        super(RBNFModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length, input_dim)\n",
    "        out, (hn, cn) = self.lstm(x)  # out shape: (batch_size, sequence_length, hidden_dim)\n",
    "        out = out[:, -1, :]  # we only use the output of the last time step\n",
    "        out = self.batch_norm(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train RMSLE = 0.8586716178261418, Test RMSLE = 0.6924337216838937 , Current LR = [0.02000080000000002]\n",
      "Epoch 2: Train RMSLE = 0.6927101926093303, Test RMSLE = 0.6900119445625881 , Current LR = [0.0400006]\n",
      "Epoch 3: Train RMSLE = 0.6909530170225817, Test RMSLE = 0.6890279623707493 , Current LR = [0.06000040000000001]\n",
      "Epoch 4: Train RMSLE = 0.6905018127347654, Test RMSLE = 0.688458244570511 , Current LR = [0.0800002]\n",
      "Epoch 5: Train RMSLE = 0.6903444401760224, Test RMSLE = 0.688870332512101 , Current LR = [0.1]\n",
      "Epoch 6: Train RMSLE = 0.6899808727512022, Test RMSLE = 0.6878011852461631 , Current LR = [0.0800002]\n",
      "Epoch 7: Train RMSLE = 0.6896246426070074, Test RMSLE = 0.6877104775682761 , Current LR = [0.06000040000000001]\n",
      "Epoch 8: Train RMSLE = 0.6891783315141721, Test RMSLE = 0.6877810799187846 , Current LR = [0.0400006]\n",
      "Epoch 9: Train RMSLE = 0.6889893982908716, Test RMSLE = 0.6873621200343288 , Current LR = [0.02000080000000002]\n",
      "Epoch 10: Train RMSLE = 0.6887345827619922, Test RMSLE = 0.68733144711649 , Current LR = [1e-06]\n",
      "Epoch 11: Train RMSLE = 0.6887006614545171, Test RMSLE = 0.6873448286012444 , Current LR = [0.02000080000000002]\n",
      "Epoch 12: Train RMSLE = 0.6890144662901955, Test RMSLE = 0.6874725796905319 , Current LR = [0.04000060000000004]\n",
      "Epoch 13: Train RMSLE = 0.6889785499142711, Test RMSLE = 0.6875546423881813 , Current LR = [0.06000039999999997]\n",
      "Epoch 14: Train RMSLE = 0.6892654247096238, Test RMSLE = 0.6876851232270578 , Current LR = [0.0800002]\n",
      "Epoch 15: Train RMSLE = 0.6892864673890378, Test RMSLE = 0.6882349577550239 , Current LR = [0.1]\n",
      "Epoch 16: Train RMSLE = 0.6894682702642518, Test RMSLE = 0.6881049323695729 , Current LR = [0.0800002]\n",
      "Epoch 17: Train RMSLE = 0.6892247665978143, Test RMSLE = 0.6877512347825907 , Current LR = [0.06000039999999997]\n",
      "Epoch 18: Train RMSLE = 0.6889027981810568, Test RMSLE = 0.6874833569115989 , Current LR = [0.04000060000000004]\n",
      "Epoch 19: Train RMSLE = 0.688724691871673, Test RMSLE = 0.6873408578948969 , Current LR = [0.02000080000000002]\n",
      "Epoch 20: Train RMSLE = 0.6885095096041574, Test RMSLE = 0.6872662537845151 , Current LR = [1e-06]\n",
      "Epoch 21: Train RMSLE = 0.6885576646408571, Test RMSLE = 0.6873948362288429 , Current LR = [0.02000080000000002]\n",
      "Epoch 22: Train RMSLE = 0.6886841467474634, Test RMSLE = 0.6874383252574892 , Current LR = [0.04000060000000004]\n",
      "Epoch 23: Train RMSLE = 0.6888737775122782, Test RMSLE = 0.687941264245291 , Current LR = [0.06000039999999997]\n",
      "Epoch 24: Train RMSLE = 0.6890597280009335, Test RMSLE = 0.6879738875418263 , Current LR = [0.0800002]\n",
      "Epoch 25: Train RMSLE = 0.6892579334022757, Test RMSLE = 0.6877276746874113 , Current LR = [0.1]\n",
      "Epoch 26: Train RMSLE = 0.6891923961044044, Test RMSLE = 0.6880636060071057 , Current LR = [0.0800002]\n",
      "Epoch 27: Train RMSLE = 0.6890296165129194, Test RMSLE = 0.6878836645738948 , Current LR = [0.06000039999999997]\n",
      "Epoch 28: Train RMSLE = 0.6886733789438889, Test RMSLE = 0.6875279529873635 , Current LR = [0.04000060000000004]\n",
      "Epoch 29: Train RMSLE = 0.6887082156652927, Test RMSLE = 0.6874531240980627 , Current LR = [0.02000080000000002]\n",
      "Epoch 30: Train RMSLE = 0.688366126482785, Test RMSLE = 0.6874208084790971 , Current LR = [1e-06]\n",
      "Minimum Train RMSLE so far: 0.688366126482785\n",
      "Minimum Test RMSLE so far: 0.6872662537845151\n"
     ]
    }
   ],
   "source": [
    "model = RBNFModel(input_dim=num_features, hidden_dim=10, output_dim=1).to(device)  # Adjust dimensions as necessary\n",
    "train_model(model, train_loader, test_loader, num_epochs=30)\n",
    "\n",
    "# Minimum Train RMSLE so far: 0.6887654505139725\n",
    "# Minimum Test RMSLE so far: 0.6873131738441437"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-tabnet\n",
      "  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from pytorch-tabnet) (1.26.4)\n",
      "Requirement already satisfied: scikit_learn>0.21 in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from pytorch-tabnet) (1.4.2)\n",
      "Requirement already satisfied: scipy>1.4 in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from pytorch-tabnet) (1.13.0)\n",
      "Requirement already satisfied: torch>=1.3 in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from pytorch-tabnet) (2.2.2+cu118)\n",
      "Requirement already satisfied: tqdm>=4.36 in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from pytorch-tabnet) (4.66.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from scikit_learn>0.21->pytorch-tabnet) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from scikit_learn>0.21->pytorch-tabnet) (3.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (2024.3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from tqdm>=4.36->pytorch-tabnet) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from jinja2->torch>=1.3->pytorch-tabnet) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\divya\\onedrive\\documents\\utaustin\\semester 6\\461p\\ece-461p-term-project\\.venv\\lib\\site-packages (from sympy->torch>=1.3->pytorch-tabnet) (1.3.0)\n",
      "Downloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n",
      "   ---------------------------------------- 0.0/44.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 44.5/44.5 kB 2.1 MB/s eta 0:00:00\n",
      "Installing collected packages: pytorch-tabnet\n",
      "Successfully installed pytorch-tabnet-4.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-tabnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\divya\\OneDrive\\Documents\\UTAUSTIN\\Semester 6\\461p\\ECE-461P-Term-Project\\.venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from pytorch_tabnet.callbacks import Callback\n",
    "\n",
    "X = train.drop(columns=['price'])\n",
    "y = np.log1p((train['price'])).values.reshape(-1, 1) # talk about this \n",
    "\n",
    "# Initialize TabNetRegressor\n",
    "model = TabNetRegressor(device_name='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.54274 | eval_rmse: 0.69856 |  0:01:17s\n",
      "epoch 1  | loss: 0.48695 | eval_rmse: 0.69605 |  0:02:16s\n",
      "epoch 2  | loss: 0.48567 | eval_rmse: 0.69344 |  0:03:10s\n",
      "epoch 3  | loss: 0.4844  | eval_rmse: 0.69511 |  0:04:02s\n",
      "epoch 4  | loss: 0.48378 | eval_rmse: 0.69366 |  0:05:17s\n",
      "epoch 5  | loss: 0.48347 | eval_rmse: 0.69348 |  0:06:35s\n",
      "epoch 6  | loss: 0.48357 | eval_rmse: 0.69595 |  0:07:52s\n",
      "epoch 7  | loss: 0.48325 | eval_rmse: 0.69243 |  0:08:37s\n",
      "epoch 8  | loss: 0.4834  | eval_rmse: 0.69331 |  0:09:24s\n",
      "epoch 9  | loss: 0.48294 | eval_rmse: 0.69339 |  0:10:09s\n",
      "epoch 10 | loss: 0.48313 | eval_rmse: 0.69269 |  0:10:56s\n",
      "epoch 11 | loss: 0.48443 | eval_rmse: 0.69336 |  0:11:42s\n",
      "epoch 12 | loss: 0.48329 | eval_rmse: 0.69556 |  0:12:27s\n",
      "epoch 13 | loss: 0.48311 | eval_rmse: 0.69327 |  0:13:13s\n",
      "epoch 14 | loss: 0.4829  | eval_rmse: 0.69274 |  0:13:58s\n",
      "epoch 15 | loss: 0.48234 | eval_rmse: 0.69239 |  0:14:44s\n",
      "epoch 16 | loss: 0.48236 | eval_rmse: 0.69248 |  0:15:32s\n",
      "epoch 17 | loss: 0.48335 | eval_rmse: 0.69442 |  0:16:16s\n",
      "epoch 18 | loss: 0.48429 | eval_rmse: 0.69228 |  0:17:02s\n",
      "epoch 19 | loss: 0.48426 | eval_rmse: 0.69366 |  0:17:48s\n",
      "epoch 20 | loss: 0.48426 | eval_rmse: 0.69382 |  0:18:34s\n",
      "epoch 21 | loss: 0.48404 | eval_rmse: 0.69421 |  0:19:22s\n",
      "epoch 22 | loss: 0.48236 | eval_rmse: 0.69228 |  0:20:06s\n",
      "epoch 23 | loss: 0.48156 | eval_rmse: 0.69244 |  0:20:53s\n",
      "epoch 24 | loss: 0.48191 | eval_rmse: 0.6921  |  0:21:39s\n",
      "epoch 25 | loss: 0.48229 | eval_rmse: 0.69346 |  0:22:25s\n",
      "epoch 26 | loss: 0.48167 | eval_rmse: 0.69225 |  0:23:12s\n",
      "epoch 27 | loss: 0.48125 | eval_rmse: 0.69178 |  0:23:57s\n",
      "epoch 28 | loss: 0.4811  | eval_rmse: 0.69211 |  0:24:43s\n",
      "epoch 29 | loss: 0.48169 | eval_rmse: 0.69294 |  0:25:30s\n",
      "epoch 30 | loss: 0.48157 | eval_rmse: 0.69175 |  0:26:15s\n",
      "epoch 31 | loss: 0.48231 | eval_rmse: 0.69357 |  0:27:02s\n",
      "epoch 32 | loss: 0.48343 | eval_rmse: 0.69386 |  0:27:48s\n",
      "epoch 33 | loss: 0.48241 | eval_rmse: 0.69359 |  0:28:33s\n",
      "epoch 34 | loss: 0.48222 | eval_rmse: 0.6931  |  0:29:21s\n",
      "epoch 35 | loss: 0.48224 | eval_rmse: 0.69338 |  0:30:06s\n",
      "epoch 36 | loss: 0.48193 | eval_rmse: 0.69205 |  0:30:53s\n",
      "epoch 37 | loss: 0.48202 | eval_rmse: 0.69259 |  0:31:40s\n",
      "epoch 38 | loss: 0.48198 | eval_rmse: 0.69226 |  0:32:24s\n",
      "epoch 39 | loss: 0.48174 | eval_rmse: 0.69218 |  0:33:10s\n",
      "epoch 40 | loss: 0.48226 | eval_rmse: 0.69329 |  0:33:55s\n",
      "epoch 41 | loss: 0.48142 | eval_rmse: 0.69198 |  0:34:42s\n",
      "epoch 42 | loss: 0.48124 | eval_rmse: 0.69215 |  0:35:28s\n",
      "epoch 43 | loss: 0.48139 | eval_rmse: 0.6924  |  0:36:13s\n",
      "epoch 44 | loss: 0.48168 | eval_rmse: 0.69233 |  0:36:59s\n",
      "epoch 45 | loss: 0.48204 | eval_rmse: 0.69287 |  0:37:46s\n",
      "epoch 46 | loss: 0.48183 | eval_rmse: 0.69367 |  0:38:31s\n",
      "epoch 47 | loss: 0.4825  | eval_rmse: 0.69264 |  0:39:19s\n",
      "epoch 48 | loss: 0.48302 | eval_rmse: 0.69424 |  0:40:04s\n",
      "epoch 49 | loss: 0.48393 | eval_rmse: 0.69484 |  0:40:50s\n",
      "epoch 50 | loss: 0.48349 | eval_rmse: 0.6943  |  0:41:36s\n",
      "epoch 51 | loss: 0.4836  | eval_rmse: 0.69378 |  0:42:22s\n",
      "epoch 52 | loss: 0.48334 | eval_rmse: 0.69382 |  0:43:09s\n",
      "epoch 53 | loss: 0.48352 | eval_rmse: 0.69391 |  0:43:54s\n",
      "epoch 54 | loss: 0.48348 | eval_rmse: 0.69377 |  0:44:40s\n",
      "epoch 55 | loss: 0.48313 | eval_rmse: 0.6936  |  0:45:27s\n",
      "epoch 56 | loss: 0.48352 | eval_rmse: 0.69384 |  0:46:12s\n",
      "epoch 57 | loss: 0.48347 | eval_rmse: 0.69401 |  0:47:00s\n",
      "epoch 58 | loss: 0.48336 | eval_rmse: 0.69425 |  0:47:46s\n",
      "epoch 59 | loss: 0.48348 | eval_rmse: 0.69381 |  0:48:31s\n",
      "epoch 60 | loss: 0.4834  | eval_rmse: 0.6935  |  0:49:18s\n",
      "epoch 61 | loss: 0.4835  | eval_rmse: 0.69392 |  0:50:03s\n",
      "epoch 62 | loss: 0.48346 | eval_rmse: 0.69444 |  0:50:49s\n",
      "epoch 63 | loss: 0.48374 | eval_rmse: 0.69443 |  0:51:37s\n",
      "epoch 64 | loss: 0.4837  | eval_rmse: 0.69526 |  0:52:21s\n",
      "epoch 65 | loss: 0.48359 | eval_rmse: 0.69377 |  0:53:08s\n",
      "epoch 66 | loss: 0.48363 | eval_rmse: 0.69594 |  0:53:54s\n",
      "epoch 67 | loss: 0.48328 | eval_rmse: 0.69403 |  0:54:40s\n",
      "epoch 68 | loss: 0.48276 | eval_rmse: 0.69339 |  0:55:27s\n",
      "epoch 69 | loss: 0.48821 | eval_rmse: 0.69927 |  0:56:12s\n",
      "epoch 70 | loss: 0.48639 | eval_rmse: 0.69429 |  0:56:57s\n",
      "epoch 71 | loss: 0.48398 | eval_rmse: 0.69375 |  0:57:43s\n",
      "epoch 72 | loss: 0.49071 | eval_rmse: 0.70861 |  0:58:28s\n",
      "epoch 73 | loss: 0.5007  | eval_rmse: 0.70718 |  0:59:16s\n",
      "epoch 74 | loss: 0.4972  | eval_rmse: 0.70486 |  1:00:01s\n",
      "epoch 75 | loss: 0.4946  | eval_rmse: 0.70153 |  1:00:48s\n",
      "epoch 76 | loss: 0.49385 | eval_rmse: 0.70192 |  1:01:35s\n",
      "epoch 77 | loss: 0.4898  | eval_rmse: 0.69756 |  1:02:21s\n",
      "epoch 78 | loss: 0.487   | eval_rmse: 0.69619 |  1:03:09s\n",
      "epoch 79 | loss: 0.48483 | eval_rmse: 0.69449 |  1:03:54s\n",
      "epoch 80 | loss: 0.48152 | eval_rmse: 0.69143 |  1:04:40s\n",
      "epoch 81 | loss: 0.48133 | eval_rmse: 0.69122 |  1:05:27s\n",
      "epoch 82 | loss: 0.48144 | eval_rmse: 0.69499 |  1:06:11s\n",
      "epoch 83 | loss: 0.48235 | eval_rmse: 0.69178 |  1:06:58s\n",
      "epoch 84 | loss: 0.4804  | eval_rmse: 0.69138 |  1:07:44s\n",
      "epoch 85 | loss: 0.4801  | eval_rmse: 0.69179 |  1:08:29s\n",
      "epoch 86 | loss: 0.48028 | eval_rmse: 0.69104 |  1:09:16s\n",
      "epoch 87 | loss: 0.48006 | eval_rmse: 0.69118 |  1:10:00s\n",
      "epoch 88 | loss: 0.47996 | eval_rmse: 0.69114 |  1:10:47s\n",
      "epoch 89 | loss: 0.47994 | eval_rmse: 0.69196 |  1:11:34s\n",
      "epoch 90 | loss: 0.48014 | eval_rmse: 0.69131 |  1:12:18s\n",
      "epoch 91 | loss: 0.47995 | eval_rmse: 0.69108 |  1:13:05s\n",
      "epoch 92 | loss: 0.48003 | eval_rmse: 0.69127 |  1:13:50s\n",
      "epoch 93 | loss: 0.48002 | eval_rmse: 0.6913  |  1:14:36s\n",
      "epoch 94 | loss: 0.48004 | eval_rmse: 0.69133 |  1:15:23s\n",
      "epoch 95 | loss: 0.48007 | eval_rmse: 0.69116 |  1:16:08s\n",
      "epoch 96 | loss: 0.47996 | eval_rmse: 0.69132 |  1:16:55s\n",
      "epoch 97 | loss: 0.47974 | eval_rmse: 0.6932  |  1:17:41s\n",
      "epoch 98 | loss: 0.47991 | eval_rmse: 0.69159 |  1:18:27s\n",
      "epoch 99 | loss: 0.47985 | eval_rmse: 0.69212 |  1:19:21s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 86 and best_eval_rmse = 0.69104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\divya\\OneDrive\\Documents\\UTAUSTIN\\Semester 6\\461p\\ECE-461P-Term-Project\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.6910353193627529\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit(\n",
    "  X_train, y_train,\n",
    "  eval_set=[(X_test, y_test)],\n",
    "  eval_name=['eval'],\n",
    "  eval_metric=['rmse'],\n",
    "  max_epochs=100,\n",
    "  patience=50,  # Early stopping based on validation loss\n",
    "  batch_size=1024,  # Adjust according to your GPU memory\n",
    "  virtual_batch_size=128,  # Mini batch size for \"Ghost Batch Normalization\"\n",
    "  num_workers=0,\n",
    "  drop_last=False\n",
    ")\n",
    "# Predict and evaluate\n",
    "predictions = model.predict(X_test)\n",
    "rmse = sqrt(mean_squared_error(y_test, predictions))\n",
    "print(f\"Test RMSE: {rmse}\")\n",
    "\n",
    "#Stop training because you reached max_epochs = 100 with best_epoch = 86 and best_eval_rmse = 0.69104 Test RMSE: 0.6910353193627529\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
